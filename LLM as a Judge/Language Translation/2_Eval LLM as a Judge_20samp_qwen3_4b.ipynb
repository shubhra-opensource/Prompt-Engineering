{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e76da3-dd58-471e-a042-ad22f5b47561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL AND PROMPT CONFIGURATION ===\n",
    "# What this does: Sets up which LLM model to use and the evaluation prompt template\n",
    "# Why we need it: We need to configure which AI model will judge translation quality\n",
    "# Input: None (configuration constants)\n",
    "# Output: MODEL_NAME and LLM_PROMPT_TEMPLATE variables used later\n",
    "\n",
    "# MODEL_NAME: Specifies which language model to use for evaluation\n",
    "# \"qwen3:4b\" = Qwen3 model with 4 billion parameters (smaller, faster)\n",
    "# \"qwen3:8b\" = Qwen3 model with 8 billion parameters (larger, potentially better quality)\n",
    "# Why: Different model sizes have different capabilities and speeds\n",
    "# Business reason: 4b model is faster/cheaper, 8b might give better evaluations\n",
    "# Technical reason: Model name must match what's available in Ollama\n",
    " # MODEL_NAME = \"qwen3:8b\"  # Alternative model (commented out)\n",
    "MODEL_NAME = \"qwen3:4b\"  # Active model: 4 billion parameter version\n",
    "\n",
    "# LLM_PROMPT_TEMPLATE: The instructions we give to the LLM to evaluate translations\n",
    "# What: A detailed prompt that tells the AI how to judge translation quality\n",
    "# Why: We need consistent, structured evaluation criteria\n",
    "# Business reason: Standardized evaluation ensures fair comparison across language pairs\n",
    "# Technical reason: LLMs need clear instructions to produce consistent outputs\n",
    "# Format: Uses {placeholders} that get filled in with actual data later\n",
    "LLM_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert multilingual linguist evaluating the QUALITY of a candidate translation.\n",
    "\n",
    "Given:\n",
    "\n",
    "Source language: {source_language}\n",
    "Target language: {target_language}\n",
    "\n",
    "Source text: {source_text}\n",
    "Target text: {target_text}\n",
    "\n",
    "Evaluate how well the TARGET TEXT translates the SOURCE TEXT.\n",
    "\n",
    "Evaluation Criteria\n",
    "\n",
    "Adequacy (Meaning Preservation)\n",
    "Is the full meaning accurately conveyed?\n",
    "Are details, nuances, logic preserved?\n",
    "Any mistranslations or factual distortions?\n",
    "\n",
    "Completeness\n",
    "Any omissions or unjustified additions?\n",
    "\n",
    "Fluency (Target Language Quality)\n",
    "Is it grammatical, natural, and idiomatic?\n",
    "\n",
    "Terminology & Entities\n",
    "Are domain terms correct and consistent?\n",
    "\n",
    "Style & Tone\n",
    "Does it match the sourceâ€™s register and intent?\n",
    "\n",
    "Structure & Formatting\n",
    "Is structure coherent?\n",
    "\n",
    "Faithfulness\n",
    "No invented facts or reinterpretations.\n",
    "\n",
    "Scoring (1â€“10)\n",
    "\n",
    "1â€“2 (Unusable)\n",
    "3â€“4 (Poor)\n",
    "5â€“6 (Fair)\n",
    "7â€“8 (Good)\n",
    "9â€“10 (Excellent)\n",
    "\n",
    "Choose ONE overall score (1â€“10).\n",
    "Do NOT reward fluency if meaning is wrong.\n",
    "\n",
    "Output Format (Strict)\n",
    "\n",
    "Total rating: <single integer from 1 to 10>\n",
    "\n",
    "Rules:\n",
    "Integer only.\n",
    "No extra fields.\n",
    "Be concise but concrete.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce6ad6d-11b9-490a-a739-040dab55a0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sp_hp\\miniconda3\\envs\\mt_benchmark\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading translation files...\n",
      "âœ… Loaded 27916 total rows\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ==========================================================\n",
    "# LLM JUDGE CONFIG\n",
    "# ==========================================================\n",
    "\n",
    "#\n",
    "\n",
    "# === IMPORT STATEMENTS ===\n",
    "# What this does: Loads all necessary libraries for LLM-based evaluation\n",
    "# Why we need it: Each library provides specific functionality for evaluating translations\n",
    "\n",
    "# pandas: Data manipulation library - works with tables/dataframes\n",
    "# Why: We need to load CSV files with translations and save evaluation results\n",
    "import pandas as pd\n",
    "\n",
    "# pathlib.Path: Modern way to handle file/folder paths (works on Windows/Mac/Linux)\n",
    "# Why: We need to read input files and write output files safely\n",
    "from pathlib import Path\n",
    "\n",
    "# ollama: Library to interact with Ollama (local LLM server)\n",
    "# Why: We use Ollama to run the Qwen model locally for translation evaluation\n",
    "import ollama\n",
    "\n",
    "# re: Regular expressions library (pattern matching in text)\n",
    "# Why: We need to extract numeric scores from LLM text responses\n",
    "import re\n",
    "\n",
    "# tqdm: Progress bar library - shows visual progress during long operations\n",
    "# Why: Evaluation takes time, so we show progress bars so users know it's working\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "# What this does: Sets up input/output file paths\n",
    "# Why we need it: Defines where to read translation data and where to save results\n",
    "# Input: None (configuration)\n",
    "# Output: Path objects for input directory and output file\n",
    "\n",
    "# input_dir: Directory containing the translation CSV files\n",
    "# Why: Centralized location where translation files from previous step are stored\n",
    "input_dir = Path(\"results\")\n",
    "input_dir.mkdir(exist_ok=True)  # ensure folder exists (creates if missing)\n",
    "\n",
    "# output_path: Where to save the evaluation results\n",
    "# Why: We need a specific file to save all evaluation scores\n",
    "# Note: Filename says \"8b\" but we're using 4b model (may be copy-paste artifact)\n",
    "output_path = input_dir / \"qwen3_8b_llm_judge_results.csv\"\n",
    "\n",
    "# === SCORE EXTRACTION FUNCTION ===\n",
    "# What this does: Extracts numeric score (1-10) from LLM text response\n",
    "# Why: LLMs return text, but we need a number to calculate statistics\n",
    "# Input: Text string (LLM response)\n",
    "# Output: Integer score (1-10) or None if no score found\n",
    "\n",
    "def extract_score(text):\n",
    "    # Regular expression pattern: \\b(10|[1-9])\\b\n",
    "    # \\b = word boundary (ensures we match whole numbers, not parts of larger numbers)\n",
    "    # (10|[1-9]) = matches either \"10\" or any digit 1-9\n",
    "    # Why: LLM might return \"Total rating: 8\" or \"Score: 9\" - we extract just the number\n",
    "    match = re.search(r\"\\b(10|[1-9])\\b\", text)\n",
    "    if match:\n",
    "        # Convert matched string to integer\n",
    "        return int(match.group(1))\n",
    "    # Return None if no score found (handles cases where LLM doesn't follow format)\n",
    "    return None\n",
    "\n",
    "\n",
    "# === LLM JUDGE FUNCTION ===\n",
    "# What this does: Calls the LLM to evaluate a single translation pair\n",
    "# Why: This is the core evaluation logic - asks AI to score translation quality\n",
    "# Business reason: Automated evaluation scales better than human evaluation\n",
    "# Input:\n",
    "#   - source_lang: Source language code (e.g., \"eng\")\n",
    "#   - target_lang: Target language code (e.g., \"jpn\")\n",
    "#   - source_text: Original text in source language\n",
    "#   - target_text: Translated text in target language\n",
    "# Output: Integer score (1-10) or None if extraction failed\n",
    "\n",
    "def call_llm_judge(source_lang, target_lang, source_text, target_text):\n",
    "\n",
    "    # Format the prompt template with actual data\n",
    "    # .format(): Replaces {placeholders} in template with actual values\n",
    "    # Why: We need to customize the prompt for each translation pair\n",
    "    # Data transformation: Template with placeholders â†’ filled prompt string\n",
    "    prompt = LLM_PROMPT_TEMPLATE.format(\n",
    "        source_language=source_lang,\n",
    "        target_language=target_lang,\n",
    "        source_text=source_text,\n",
    "        target_text=target_text\n",
    "    )\n",
    "\n",
    "    # Call Ollama API to get LLM response\n",
    "    # model=MODEL_NAME: Which model to use (qwen3:4b)\n",
    "    # messages: Conversation format - single user message with our prompt\n",
    "    # options={\"temperature\": 0}: Set randomness to 0 (deterministic, consistent outputs)\n",
    "    #   Why: We want consistent scores, not random variations\n",
    "    #   Technical reason: temperature=0 makes model always pick most likely response\n",
    "    # Input: Prompt string\n",
    "    # Output: Dictionary with LLM response\n",
    "    response = ollama.chat(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        options={\"temperature\": 0}\n",
    "    )\n",
    "\n",
    "    # Extract the actual text content from response\n",
    "    # response[\"message\"][\"content\"]: Gets the text the LLM generated\n",
    "    # .strip(): Removes leading/trailing whitespace\n",
    "    # Why: Clean up the response before extracting score\n",
    "    output = response[\"message\"][\"content\"].strip()\n",
    "    \n",
    "    # Extract numeric score from the text response\n",
    "    score = extract_score(output)\n",
    "\n",
    "    # Return the score (or None if extraction failed)\n",
    "    return score\n",
    "\n",
    "\n",
    "# === LOAD CSV FILES ===\n",
    "# What this does: Loads translation data from CSV files created in previous step\n",
    "# Why we need it: We need the translations to evaluate them\n",
    "# Input: CSV files in \"results\" folder\n",
    "# Output: Combined DataFrame with all translations\n",
    "\n",
    "print(\"ðŸ“‚ Loading translation files...\")\n",
    "\n",
    "# Load English source translations\n",
    "# Input: CSV file with columns: source_language, target_language, original_text, translated_text\n",
    "# Output: DataFrame with English â†’ all target languages translations\n",
    "eng_df = pd.read_csv(input_dir / \"english_translations.csv\")\n",
    "\n",
    "# Load French source translations\n",
    "# Input: CSV file with same structure but French source texts\n",
    "# Output: DataFrame with French â†’ all target languages translations\n",
    "fra_df = pd.read_csv(input_dir / \"french_translations.csv\")\n",
    "\n",
    "# Combine both dataframes into one\n",
    "# pd.concat(): Stacks dataframes vertically (one on top of the other)\n",
    "# ignore_index=True: Resets row numbers (0, 1, 2... instead of keeping original indices)\n",
    "# Why: We want all translations in one place for easier processing\n",
    "# Data transformation: Two separate DataFrames â†’ One combined DataFrame\n",
    "# Input: eng_df and fra_df\n",
    "# Output: full_df with all translation pairs\n",
    "full_df = pd.concat([eng_df, fra_df], ignore_index=True)\n",
    "\n",
    "print(f\"âœ… Loaded {len(full_df)} total rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649b20f6-381c-47a6-914e-c1dd2f05ad95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING LLM-AS-JUDGE EVALUATION\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Language Pairs:   0%|                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Evaluating eng â†’ dan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "eng->dan:   0%|                                                                                 | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# === EVALUATION PHASE ===\n",
    "# What this does: Evaluates translation quality for each language pair using LLM judge\n",
    "# Why we need it: This is the main evaluation - scores each translation pair\n",
    "# Business reason: Need quantitative scores to compare translation quality across languages\n",
    "# Input: Combined DataFrame with all translations\n",
    "# Output: CSV file with mean scores per language pair\n",
    "\n",
    "# Initialize list to store evaluation results\n",
    "# Why: We'll collect scores for each language pair before saving\n",
    "# Structure: List of dictionaries, each with language pair and mean score\n",
    "evaluation_results = []\n",
    "\n",
    "# Group translations by source language and target language\n",
    "# groupby(): Splits dataframe into groups based on column values\n",
    "# Why: We want to evaluate each language pair separately (e.g., engâ†’jpn, fraâ†’jpn)\n",
    "# Input: DataFrame with source_language and target_language columns\n",
    "# Output: Grouped object that can iterate over (src_lang, tgt_lang) pairs\n",
    "grouped = full_df.groupby([\"source_language\", \"target_language\"])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STARTING LLM-AS-JUDGE EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Loop through each language pair (e.g., engâ†’jpn, engâ†’kor, fraâ†’jpn, etc.)\n",
    "# Why: We need to evaluate each sourceâ†’target combination separately\n",
    "# tqdm: Shows progress bar for all language pairs\n",
    "for (src_lang, tgt_lang), group in tqdm(grouped, desc=\"Language Pairs\"):\n",
    "\n",
    "    print(f\"\\nðŸ“Š Evaluating {src_lang} â†’ {tgt_lang}\")\n",
    "\n",
    "    # Initialize list to store scores for this language pair\n",
    "    scores = []\n",
    "\n",
    "    # Sample size: Use up to 20 translations per language pair\n",
    "    # min(20, len(group)): Use 20 if available, otherwise use all available\n",
    "    # Why: Evaluating all translations would be too slow/expensive\n",
    "    # Business reason: Sampling reduces cost and time while maintaining statistical validity\n",
    "    # Technical reason: 20 samples is often enough for reliable mean estimates\n",
    "    sample_size = min(20, len(group))\n",
    "    \n",
    "    # Randomly sample translations from this language pair\n",
    "    # .sample(): Randomly selects rows from the group\n",
    "    # random_state=42: Sets random seed for reproducibility (same sample each run)\n",
    "    #   Why: Reproducible results - same translations evaluated each time\n",
    "    # Input: Group of translations for this language pair\n",
    "    # Output: Sampled rows (up to 20 translations)\n",
    "    sampled_rows = group.sample(sample_size, random_state=42)\n",
    "\n",
    "    # Loop through each sampled translation\n",
    "    # iterrows(): Iterates over DataFrame rows (index, row data)\n",
    "    # tqdm: Shows progress bar for this language pair\n",
    "    # leave=False: Don't keep progress bar after completion (cleaner output)\n",
    "    for _, row in tqdm(\n",
    "        sampled_rows.iterrows(),\n",
    "        total=sample_size,\n",
    "        desc=f\"{src_lang}->{tgt_lang}\",\n",
    "        leave=False\n",
    "    ):\n",
    "\n",
    "        # Call LLM to evaluate this translation\n",
    "        # Input: Language codes and texts from current row\n",
    "        # Output: Score (1-10) or None if extraction failed\n",
    "        score = call_llm_judge(\n",
    "            row[\"source_language\"],\n",
    "            row[\"target_language\"],\n",
    "            row[\"original_text\"],\n",
    "            row[\"translated_text\"]\n",
    "        )\n",
    "\n",
    "        # Only add valid scores (skip None values)\n",
    "        # Why: Some LLM responses might not contain extractable scores\n",
    "        if score is not None:\n",
    "            scores.append(score)\n",
    "\n",
    "    # Check if we got any valid scores\n",
    "    # Why: Can't calculate mean if no scores were extracted\n",
    "    if len(scores) == 0:\n",
    "        print(\"âš  No valid scores for this pair\")\n",
    "        continue  # Skip to next language pair\n",
    "\n",
    "    # Calculate mean (average) score for this language pair\n",
    "    # sum(scores): Add all scores together\n",
    "    # len(scores): Count of scores\n",
    "    # round(..., 3): Round to 3 decimal places (e.g., 8.333)\n",
    "    # Why: Mean score represents overall quality for this language pair\n",
    "    # Data transformation: List of scores â†’ single mean value\n",
    "    avg_score = round(sum(scores) / len(scores), 3)\n",
    "\n",
    "    # Create result dictionary for this language pair\n",
    "    # Why: Structured data format for saving to CSV\n",
    "    result_row = {\n",
    "        \"source_language\": src_lang,        # Which source language\n",
    "        \"target_language\": tgt_lang,        # Which target language\n",
    "        \"num_samples\": len(scores),         # How many translations were evaluated\n",
    "        \"mean_llm_judge_score\": avg_score   # Average quality score (1-10)\n",
    "    }\n",
    "\n",
    "    # Add this result to our collection\n",
    "    evaluation_results.append(result_row)\n",
    "\n",
    "    # âœ… PRINT RESULT IMMEDIATELY\n",
    "    # Why: User can see progress in real-time (helpful for long runs)\n",
    "    print(\"\\nâœ… RESULT\")\n",
    "    print(f\"Language Pair : {src_lang} â†’ {tgt_lang}\")\n",
    "    print(f\"Samples Used  : {len(scores)}\")\n",
    "    print(f\"Mean Score    : {avg_score}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # âœ… SAVE AFTER EACH LANGUAGE PAIR\n",
    "    # Why: Incremental saving - if script crashes, we don't lose all progress\n",
    "    # Convert results list to DataFrame\n",
    "    # Input: List of dictionaries\n",
    "    # Output: DataFrame with columns: source_language, target_language, num_samples, mean_llm_judge_score\n",
    "    results_df = pd.DataFrame(evaluation_results)\n",
    "    # Save to CSV (overwrites file each time with updated results)\n",
    "    # index=False: Don't save row numbers\n",
    "    # Expected output location: results/qwen3_8b_llm_judge_results.csv\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "\n",
    "# === FINAL SUMMARY ===\n",
    "# What this does: Prints completion message with file location\n",
    "# Why: User needs to know where to find the final results\n",
    "# Input: None\n",
    "# Output: Console message\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸŽ‰ EVALUATION COMPLETE\")\n",
    "print(f\"ðŸ“„ Results saved to: {output_path}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b909a1-f7db-41c6-9c0d-8a9ee3c3168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bb40c2-aa2a-4652-90a3-ed37d8b2ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe74217-9d97-40a3-bfb0-822a1c4d500b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mt_bench)",
   "language": "python",
   "name": "mt_bench"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
