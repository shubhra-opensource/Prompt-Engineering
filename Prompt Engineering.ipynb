{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Psl09lEOh8Wh"
   },
   "source": [
    "# üöÄ Prompt Engineering: Comprehensive Testing & Optimization Guide\n",
    "\n",
    "This notebook demonstrates advanced prompt engineering techniques for Large Language Models (LLMs), including:\n",
    "- **Pointwise vs. Pairwise Testing** - Single prompt evaluation vs. comparative analysis\n",
    "- **Reference-Based vs. Reference-Free Evaluation** - Ground truth comparison vs. LLM-based assessment\n",
    "- **Parameter Tuning** - Temperature and Top-P effects on model outputs\n",
    "- **Advanced Techniques** - Few-shot learning, Chain-of-Thought, structured outputs, and more\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Prompt Testing Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VhZRmRHcinKu",
    "outputId": "23d3d259-8d8d-4a27-e774-5af3b73c9791"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sp_hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Import required libraries for prompt engineering experiments.\n",
    "\"\"\"\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")  # Set visualization style\n",
    "import matplotlib.pyplot as plt\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK data (punkt for tokenization, wordnet for METEOR)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Q2jnVwI_ipze"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialize HuggingFace authentication and model clients.\n",
    "\"\"\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import re  # Import re module for regex operations (used in extract_judge_score)\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "hf_token = os.getenv(\"HF_TOKEN\")  # Retrieve HuggingFace API token\n",
    "\n",
    "# Initialize DeepSeek model client for primary experiments\n",
    "# DeepSeek-R1-Distill-Qwen-32B is a reasoning model with redacted reasoning\n",
    "deepseek_model_client = InferenceClient(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n",
    "    token=hf_token,\n",
    "    # headers={\"X-Use-Cache\": \"false\"}  # Uncomment to disable caching\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3KrUXdvd4nXn"
   },
   "outputs": [],
   "source": [
    "def generate_response(model, system_role, user_query, temperature = 0.1, top_p = 0.1):\n",
    "\n",
    "    response = model.chat_completion(\n",
    "    messages=[{\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": user_query}],\n",
    "    max_tokens=4000,\n",
    "    temperature = temperature,\n",
    "    top_p = top_p\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WsF6NCY4jBQ"
   },
   "source": [
    "## üìä Pointwise vs. Pairwise Testing\n",
    "\n",
    "**Pointwise Testing:** Evaluates a single prompt in isolation to understand its baseline performance.\n",
    "\n",
    "**Pairwise Testing:** Compares multiple prompt variations side-by-side to identify which performs better.\n",
    "\n",
    "**Key Difference:** Pointwise gives absolute performance; Pairwise gives relative comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbPQiPCrjpTM"
   },
   "source": [
    "### üìç Pointwise Testing\n",
    "\n",
    "Evaluates a single prompt without comparison. Useful for:\n",
    "- Initial prompt development\n",
    "- Understanding baseline performance\n",
    "- Testing individual prompt variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "qTOaKVSx4olp",
    "outputId": "b57451e1-2417-420d-8c35-71ec64e3df50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sentiment of the tweet is positive. The user expresses enjoyment of the movie, which is a strong positive sentiment, despite mentioning a minor criticism about its length. The overall tone reflects a favorable opinion of the movie.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pointwise Testing Example: Single Prompt Evaluation\n",
    "\n",
    "This example demonstrates evaluating one prompt in isolation.\n",
    "The model analyzes sentiment of a tweet without format constraints.\n",
    "\"\"\"\n",
    "\n",
    "# Define the system role (model's persona/instructions)\n",
    "system_role = \"You are an expert tweet sentiment analyzer.\"\n",
    "\n",
    "# User query: sentiment analysis task without format constraints\n",
    "user_query = f\"\"\"What is the sentiment expressed in the following tweet:\n",
    "I like the movie but it was a bit too long.\"\"\"\n",
    "\n",
    "# Generate response using the model\n",
    "output = generate_response(\n",
    "    deepseek_model_client,\n",
    "    system_role,\n",
    "    user_query\n",
    ")\n",
    "\n",
    "# DeepSeek-R1 includes reasoning tokens; extract only the final response\n",
    "# The model outputs: <reasoning>...</reasoning></think>final_answer\n",
    "response = output.strip().split(\"</think>\")[-1].strip()\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_26u2o0jr9m"
   },
   "source": [
    "### üîÑ Pairwise Testing\n",
    "\n",
    "Compares multiple prompt variations to identify the most effective approach.\n",
    "Useful for:\n",
    "- Prompt optimization\n",
    "- A/B testing different formulations\n",
    "- Understanding how format constraints affect outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4qQpDVZT49fj",
    "outputId": "e5654e46-c4ee-445e-c075-87e5722b6255"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Prompt 1: The sentiment of the tweet is **positive**. While the user acknowledges a negative aspect (the movie being too long), the overall sentiment is dominated by the positive statement that they like the movie.\n",
      "Response from Prompt 2: mixed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pairwise Testing Example: Comparing Two Prompt Variations\n",
    "\n",
    "This example compares:\n",
    "1. Prompt without format constraints (free-form response)\n",
    "2. Prompt with explicit format constraints (one-word output)\n",
    "\n",
    "This demonstrates how adding format instructions changes model behavior.\n",
    "\"\"\"\n",
    "\n",
    "system_role = \"You are an expert tweet sentiment analyzer.\"\n",
    "\n",
    "# Prompt 1: No format constraints - model can respond freely\n",
    "user_query1 = f\"\"\"What is the sentiment expressed in the following tweet:\n",
    "I like the movie but it was a bit too long.\"\"\"\n",
    "\n",
    "# Prompt 2: Explicit format constraint - must be one word from specified options\n",
    "# Note: \"postive\" is a typo in the original prompt (should be \"positive\")\n",
    "user_query2 = f\"\"\"What is the sentiment expressed in the following tweet.\n",
    "Your response must be one word: postive, negative, or mixed.\n",
    "I like the movie but it was a bit too long.\"\"\"\n",
    "\n",
    "# Store prompts in dictionary for easy iteration\n",
    "prompts = {\n",
    "    \"Prompt 1\": user_query1,  # Free-form response\n",
    "    \"Prompt 2\": user_query2   # Constrained format\n",
    "}\n",
    "\n",
    "# Generate and compare responses from both prompts\n",
    "for prompt, user_query in prompts.items():\n",
    "    output = generate_response(\n",
    "        deepseek_model_client,\n",
    "        system_role,\n",
    "        user_query\n",
    "    )\n",
    "    \n",
    "    # Extract final response (excluding reasoning tokens)\n",
    "    response = output.strip().split(\"</think>\")[-1].strip()\n",
    "    print(f\"Response from {prompt}: {response}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cg-gwO1X9xW4"
   },
   "source": [
    "## üéØ Reference-Free vs. Reference-Based Testing\n",
    "\n",
    "**Reference-Based Testing:** Compares LLM outputs against ground truth labels or expected responses.\n",
    "- ‚úÖ Objective and measurable\n",
    "- ‚ùå Requires labeled data\n",
    "\n",
    "**Reference-Free Testing:** Uses another LLM or heuristics to evaluate responses without predefined answers.\n",
    "- ‚úÖ No labels needed, flexible\n",
    "- ‚ùå Subjective, may be inconsistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2icohN1jz28"
   },
   "source": [
    "### ‚úÖ Reference-Based Testing\n",
    "\n",
    "Evaluates responses against known ground truth labels.\n",
    "Perfect for tasks with clear correct answers (classification, fact-checking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-y6h9wWQ-QC8",
    "outputId": "49463e1d-b9bc-4528-a03a-ce842d68f9b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixed\n",
      "‚úÖ Correct\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reference-Based Testing Example: Ground Truth Comparison\n",
    "\n",
    "This example evaluates the model's response against a known correct answer.\n",
    "The target label \"mixed\" represents the ground truth for this tweet.\n",
    "\"\"\"\n",
    "\n",
    "# Ground truth label (what we expect the model to predict)\n",
    "target_label = \"mixed\"\n",
    "\n",
    "system_role = \"You are an expert tweet sentiment analyzer.\"\n",
    "user_query = f\"\"\"What is the sentiment expressed in the following tweet.\n",
    "Your response must be one word: postive, negative, or mixed.\n",
    "I like the movie but it was a bit too long.\"\"\"\n",
    "\n",
    "# Generate model response\n",
    "output = generate_response(\n",
    "    deepseek_model_client,\n",
    "    system_role,\n",
    "    user_query\n",
    ")\n",
    "\n",
    "# Extract final response\n",
    "response = output.strip().split(\"</think>\")[-1].strip()\n",
    "print(response)\n",
    "\n",
    "# Compare against ground truth\n",
    "if response == target_label:\n",
    "    print(\"‚úÖ Correct\")\n",
    "else:\n",
    "    print(\"‚ùå Incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aJ7vsZdj6N1"
   },
   "source": [
    "### ü§ñ Reference-Free Testing\n",
    "\n",
    "Uses another LLM (judge model) to evaluate responses without ground truth.\n",
    "Useful when labels are unavailable or evaluation criteria are subjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "A4tDJKo5-oFN",
    "outputId": "cebb189a-1cb0-4cb3-f2fb-7951b3f4c5ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes. \\n\\nThe tweet expresses both a positive sentiment (\"I like the movie\") and a negative sentiment (\"it was a bit too long\"), which justifies the response \"mixed\".'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reference-Free Testing Example: LLM-as-a-Judge\n",
    "\n",
    "This example uses a separate LLM (Llama-3.3-70B) to evaluate the response\n",
    "from the primary model (DeepSeek) without requiring ground truth labels.\n",
    "\n",
    "This is a common pattern in production systems where human evaluation\n",
    "is expensive or labels are unavailable.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize a different model to act as a judge/evaluator\n",
    "# Using Llama-3.3-70B-Instruct as the judge model\n",
    "llama_model_client = InferenceClient(\n",
    "    \"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "# Judge model's role: evaluate other LLM responses\n",
    "system_role = \"You are an expert LLM response evaluator.\"\n",
    "\n",
    "# Judge prompt: asks the judge model to evaluate accuracy\n",
    "# Note: Uses variables from previous cell (user_query, response)\n",
    "user_query = f\"\"\"Given the following input to an LLM: {user_query},\n",
    "and the following response {response}. Do you think the response is accurate?\"\"\"\n",
    "\n",
    "# Generate judge's evaluation\n",
    "output = generate_response(\n",
    "    llama_model_client,\n",
    "    system_role,\n",
    "    user_query\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xb1N0bgDEt1I"
   },
   "source": [
    "## ‚öôÔ∏è Factors Affecting Prompt Response\n",
    "\n",
    "Several factors influence LLM outputs beyond just the prompt text:\n",
    "1. **System Instructions** - Role definition and persona\n",
    "2. **Temperature** - Controls randomness in sampling\n",
    "3. **Top-P** - Nucleus sampling parameter for diversity control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRA_69sffJnr"
   },
   "source": [
    "### üë§ System Instructions\n",
    "\n",
    "The system role/persona significantly influences output style, format, and behavior.\n",
    "More specific instructions lead to more constrained, predictable outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "xt6bW199fPiI",
    "outputId": "9ebab07b-2154-45e6-ff9f-b4815622effe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sentiment expressed in the tweet is mixed. The user likes the movie (positive) but finds it too long (negative), indicating a combination of both sentiments. \\n\\nAnswer: Mixed'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "System Instructions Example 1: Vague Role Definition\n",
    "\n",
    "This example uses a basic system role without format constraints.\n",
    "The model may provide verbose or free-form responses.\n",
    "\"\"\"\n",
    "\n",
    "# Basic system role - no format constraints\n",
    "system_role = \"You are an expert tweet sentiment analyzer.\"\n",
    "\n",
    "# User query with format instruction (note: typo \"postive\" instead of \"positive\")\n",
    "user_query = f\"\"\"What is the sentiment expressed in the following tweet.\n",
    "Your response must be negative postive, negative, or mixed.\n",
    "I like the movie but it was a bit too long.\"\"\"\n",
    "\n",
    "output = generate_response(\n",
    "    deepseek_model_client,\n",
    "    system_role,\n",
    "    user_query\n",
    ")\n",
    "\n",
    "response = output.strip().split(\"</think>\")[-1].strip()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "k3mTMEORfmUs",
    "outputId": "7ed917fa-8bfd-426a-bb1d-576aead8b2e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mixed'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "System Instructions Example 2: Specific Format Constraint\n",
    "\n",
    "This example adds format constraint to the system role itself.\n",
    "Combining system-level and user-level constraints for stronger enforcement.\n",
    "\"\"\"\n",
    "\n",
    "# Enhanced system role with explicit format constraint\n",
    "system_role = \"You are an expert tweet sentiment analyzer. You respond in a single word.\"\n",
    "\n",
    "# Same user query as before\n",
    "user_query = f\"\"\"What is the sentiment expressed in the following tweet.\n",
    "Your response must be negative postive, negative, or mixed.\n",
    "I like the movie but it was a bit too long.\"\"\"\n",
    "\n",
    "output = generate_response(\n",
    "    deepseek_model_client,\n",
    "    system_role,\n",
    "    user_query\n",
    ")\n",
    "\n",
    "response = output.strip().split(\"</think>\")[-1].strip()\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDLOkwnHG6iH"
   },
   "source": [
    "### üå°Ô∏è Temperature Settings\n",
    "\n",
    "Temperature controls randomness in token sampling:\n",
    "- **Low (0.0-0.3):** Deterministic, consistent outputs (best for factual tasks)\n",
    "- **Medium (0.5-0.7):** Balanced creativity and consistency\n",
    "- **High (0.8-1.0+):** Creative, varied outputs (best for creative writing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "liuFF2SbExGD",
    "outputId": "dcbecd13-1263-422d-b38a-189dd879becb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sentiment expressed in the tweet is mixed. The user liked the movie, indicating a positive sentiment, but also found it too long, which introduces a negative aspect. Therefore, the overall sentiment is a combination of both positive and negative elements.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Temperature Example 1: Low Temperature (Deterministic)\n",
    "\n",
    "Temperature = 0.0 makes the model choose the most likely token at each step.\n",
    "This produces consistent, deterministic outputs - same prompt ‚Üí same response.\n",
    "Best for: Factual tasks, classification, when consistency is critical.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = \"You are an expert tweet sentiment analyzer.\"\n",
    "user_query = f\"\"\"What is the sentiment expressed in the following tweet:\n",
    "I liked the movie but it was a bit too long.\"\"\"\n",
    "\n",
    "# Temperature = 0: Most deterministic, always picks highest probability token\n",
    "output = generate_response(\n",
    "    deepseek_model_client,\n",
    "    system_prompt,\n",
    "    user_query,\n",
    "    temperature=0  # Deterministic sampling\n",
    ")\n",
    "\n",
    "response = output.strip().split(\"</think>\")[-1].strip()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "AgFhiTNsFC5i",
    "outputId": "7bcdc509-b180-4418-c274-17b10bfd3293"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sentiment of the tweet is mixed. The user expresses a positive sentiment by liking the movie but also mentions a negative aspect regarding its length. Therefore, the overall sentiment is mixed.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Temperature Example 2: High Temperature (Creative)\n",
    "\n",
    "Temperature = 0.8 introduces randomness, allowing less likely tokens to be selected.\n",
    "This produces more varied, creative outputs - same prompt ‚Üí different responses.\n",
    "Best for: Creative writing, brainstorming, when diversity is desired.\n",
    "\"\"\"\n",
    "\n",
    "# Same prompt as before, but with higher temperature\n",
    "output = generate_response(\n",
    "    deepseek_model_client,\n",
    "    system_prompt,\n",
    "    user_query,\n",
    "    temperature=0.8  # Higher randomness, more creative\n",
    ")\n",
    "\n",
    "response = output.strip().split(\"</think>\")[-1].strip()\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TV3klLeeHCrH"
   },
   "source": [
    "### üé≤ The Effect of Top-P (Nucleus Sampling)\n",
    "\n",
    "Top-P limits token selection to the top-p probability mass:\n",
    "- **Low (0.1-0.3):** Very focused, narrow vocabulary (precise outputs)\n",
    "- **Medium (0.5-0.7):** Balanced diversity\n",
    "- **High (0.8-0.95):** Diverse vocabulary, creative responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "6z7OlJqCXobw",
    "outputId": "9ab86c74-d237-4630-ed04-5c13b7409505"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sentiment of the tweet is mixed. The user expresses a positive sentiment by stating they liked the movie but also includes a negative aspect by mentioning that it was a bit too long. Therefore, the overall sentiment combines both positive and negative elements.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Top-P Example 1: Low Top-P (Focused Sampling)\n",
    "\n",
    "Top-P = 0.1 means only tokens in the top 10% probability mass are considered.\n",
    "This creates very focused, predictable outputs with limited vocabulary diversity.\n",
    "Works well with temperature = 0.5 for balanced control.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = \"You are an expert tweet sentiment analyzer.\"\n",
    "user_query = f\"\"\"What is the sentiment expressed in the following tweet:\n",
    "I liked the movie but it was a bit too long.\"\"\"\n",
    "\n",
    "# Low top_p: Only considers tokens in top 10% probability mass\n",
    "output = generate_response(\n",
    "    deepseek_model_client,\n",
    "    system_prompt,\n",
    "    user_query,\n",
    "    temperature=0.5,  # Moderate randomness\n",
    "    top_p=0.1         # Very focused vocabulary\n",
    ")\n",
    "\n",
    "response = output.strip().split(\"</think>\")[-1].strip()\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "R2b21Nm_dL5H",
    "outputId": "f4edf61d-941b-49c6-f40e-4a98aa506d36"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sentiment of the tweet \"I liked the movie but it was a bit too long\" is mixed. This classification is because the tweet contains both positive and negative elements. The user expresses enjoyment of the movie, indicating a positive sentiment, but also criticizes its length, which introduces a negative aspect. Therefore, the overall sentiment is mixed, reflecting both appreciation and criticism.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Top-P Example 2: High Top-P (Diverse Sampling)\n",
    "\n",
    "Top-P = 0.9 means tokens in the top 90% probability mass are considered.\n",
    "This allows for more diverse vocabulary and creative word choices.\n",
    "Compare with previous cell to see the difference in output style.\n",
    "\"\"\"\n",
    "\n",
    "# Same prompt, but with higher top_p for more diversity\n",
    "output = generate_response(\n",
    "    deepseek_model_client,\n",
    "    system_prompt,\n",
    "    user_query,\n",
    "    temperature=0.5,  # Same temperature\n",
    "    top_p=0.9         # Much broader vocabulary selection\n",
    ")\n",
    "\n",
    "response = output.strip().split(\"</think>\")[-1].strip()\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-q6Qdjc7u__"
   },
   "source": [
    "### ‚öñÔ∏è LLM As a Judge\n",
    "\n",
    "Using a separate LLM to evaluate responses from another model.\n",
    "This enables scalable evaluation without human annotators.\n",
    "The judge provides structured feedback with ratings and rationale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "id": "OjKoOq-6jGMs",
    "outputId": "1d45cad5-68ae-4647-aa39-c541a9e1c0f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Building a machine learning model to predict customer churn involves several key steps, each with its own considerations and challenges. Here's a structured approach to guide you through the process:\\n\\n### 1. Data Preparation\\n- **Data Collection**: Gather relevant data from company databases or CRM systems, including subscription details, payment history, usage patterns, demographics, and support interactions.\\n- **Data Cleaning**: Address missing values, duplicates, and outliers. Use imputation techniques for missing data and remove duplicates to avoid bias.\\n- **Handling Categorical Variables**: Convert categorical variables using one-hot encoding for nominal data and label encoding for ordinal data.\\n\\n### 2. Feature Engineering\\n- **Feature Creation**: Develop new features from existing data, such as average monthly usage or support tickets filed, to capture hidden patterns.\\n- **Data Splitting**: Split the dataset into training, validation, and test sets. Consider stratified sampling if churn is rare to maintain class distribution.\\n\\n### 3. Model Selection\\n- **Algorithm Choice**: Experiment with algorithms like logistic regression, random forests, and gradient boosting. Evaluate based on interpretability and accuracy.\\n- **Hyperparameter Tuning**: Use grid or random search to optimize parameters, starting with the most impactful ones.\\n\\n### 4. Model Evaluation\\n- **Performance Metrics**: Use accuracy, precision, recall, F1-score, and AUC-ROC. Focus on precision and recall for imbalanced classes.\\n- **Interpretability**: Employ SHAP values or LIME to explain model decisions, aiding stakeholders in understanding churn drivers.\\n\\n### 5. Deployment and Monitoring\\n- **Integration**: Deploy the model into business processes, such as alerting support teams of potential churn.\\n- **Monitoring**: Continuously monitor performance and retrain with new data to adapt to changing customer behavior.\\n\\n### Additional Considerations\\n- **Class Imbalance**: Address using SMOTE or class weight adjustment.\\n- **Feature Scaling**: Apply as needed for specific algorithms.\\n\\nBy following these steps, you can build an effective churn prediction model, ensuring accuracy and adaptability over time.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_role = \"You are an expert in Question Answering. Answer like a human.\"\n",
    "content=\"\"\"I am building a machine learning model to predict customer churn for a subscription-based service.\n",
    "Can you explain the key steps involved in building such a model, including data preparation,\n",
    "feature engineering, model selection, and evaluation? \"\"\"\n",
    "user_query = f\"\"\"Generate a answer for the following question in 1000 characters:\\n{content}\"\"\"\n",
    "output = generate_response(deepseek_model_client,\n",
    "                           system_role,\n",
    "                           user_query)\n",
    "\n",
    "## only retrieve the response not the thought process\n",
    "response1 = output.strip().split(\"</think>\")[-1].strip()\n",
    "response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_judge_score(answer: str, split_str: str = \"Total rating:\") -> int:\n",
    "    try:\n",
    "        if split_str in answer:\n",
    "            rating = answer.split(split_str)[1]\n",
    "        else:\n",
    "            rating = answer\n",
    "        digit_groups = [el.strip() for el in re.findall(r\"\\d+(?:\\.\\d+)?\", rating)]\n",
    "        return float(digit_groups[0])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "IMPROVED_JUDGE_PROMPT = \"\"\"\n",
    "You will be given a user_question and system_answer couple.\n",
    "Your task is to provide a 'total rating' scoring how well the system_answer answers the user concerns expressed in the user_question.\n",
    "Give your answer on a scale of 1 to 4, where 1 means that the system_answer is not helpful at all, and 4 means that the system_answer completely and helpfully addresses the user_question.\n",
    "\n",
    "Here is the scale you should use to build your answer:\n",
    "1: The system_answer is terrible: completely irrelevant to the question asked, or very partial\n",
    "2: The system_answer is mostly not helpful: misses some key aspects of the question\n",
    "3: The system_answer is mostly helpful: provides support, but still could be improved\n",
    "4: The system_answer is excellent: relevant, direct, detailed, and addresses all the concerns raised in the question\n",
    "\n",
    "Provide your feedback as follows:\n",
    "\n",
    "Feedback:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 4)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and answer.\n",
    "\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "\n",
    "Provide your feedback. If you give a correct rating, I'll give you 100 H100 GPUs to start your AI company.\n",
    "Feedback:::\n",
    "Evaluation: \"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6. Judge the Answer\n",
    "# -----------------------------\n",
    "judge_prompt = IMPROVED_JUDGE_PROMPT.format(\n",
    "    question=content,\n",
    "    answer=response1\n",
    ")\n",
    "\n",
    "# judge_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Feedback:::\\nEvaluation: The system_answer provides a comprehensive and structured approach to building a machine learning model for predicting customer churn. It covers all the key steps involved, including data preparation, feature engineering, model selection, and evaluation, as well as additional considerations such as class imbalance and feature scaling. The answer is detailed, direct, and relevant to the question asked, addressing all the concerns raised. The use of specific techniques and algorithms, such as one-hot encoding, stratified sampling, and SHAP values, demonstrates a high level of expertise and provides actionable advice for the user. Overall, the system_answer is excellent, providing a clear and thorough guide for building an effective churn prediction model.\\n\\nTotal rating: 4'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_role = \"You are an expert judge response evaulator.\"\n",
    "user_query = judge_prompt\n",
    "\n",
    "\n",
    "judge_output = generate_response(llama_model_client,\n",
    "               system_role,\n",
    "               user_query)\n",
    "judge_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge Feedback:\n",
      "Feedback:::\n",
      "Evaluation: The system_answer provides a comprehensive and structured approach to building a machine learning model for predicting customer churn. It covers all the key steps involved, including data preparation, feature engineering, model selection, and evaluation, as well as additional considerations such as class imbalance and feature scaling. The answer is detailed, direct, and relevant to the question asked, addressing all the concerns raised. The use of specific techniques and algorithms, such as one-hot encoding, stratified sampling, and SHAP values, demonstrates a high level of expertise and provides actionable advice for the user. Overall, the system_answer is excellent, providing a clear and thorough guide for building an effective churn prediction model.\n",
      "\n",
      "Total rating: 4\n",
      "--------------------------------------------------\n",
      "Final LLM-as-a-Judge Score: 4.0\n"
     ]
    }
   ],
   "source": [
    "judge_score = extract_judge_score(judge_output)\n",
    "\n",
    "print(\"Judge Feedback:\")\n",
    "print(judge_output)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(f\"Final LLM-as-a-Judge Score: {judge_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Advanced Prompting Techniques\n",
    "\n",
    "### üéì Few-Shot Learning\n",
    "\n",
    "Demonstrates in-context learning by providing examples before the target task.\n",
    "Few-shot prompts improve accuracy by clarifying expected format and task boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ZERO-SHOT ===\n",
      "Response: The sentiment of the review is negative. The customer expresses dissatisfaction with both the damaged product and the unhelpful customer service.\n",
      "\n",
      "=== FEW-SHOT ===\n",
      "Response: The sentiment of the review is negative because it highlights two negative experiences: the product arriving damaged and unhelpful customer service, without any positive elements.\n",
      "\n",
      "Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Few-Shot Learning: Zero-Shot vs. Few-Shot Comparison\n",
    "\n",
    "This example demonstrates the difference between:\n",
    "1. Zero-Shot: No examples provided, model relies on pre-training\n",
    "2. Few-Shot: Examples provided in-context to guide model behavior\n",
    "\n",
    "Few-shot learning often improves accuracy by:\n",
    "- Clarifying the expected output format\n",
    "- Demonstrating task boundaries\n",
    "- Providing domain-specific patterns\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# Zero-Shot Example: No examples provided\n",
    "# ============================================================================\n",
    "zero_shot_prompt = \"\"\"Classify the sentiment of the following review as positive, negative, or neutral:\n",
    "Review: The product arrived damaged and the customer service was unhelpful.\"\"\"\n",
    "\n",
    "print(\"=== ZERO-SHOT ===\")\n",
    "output_zero = generate_response(\n",
    "    deepseek_model_client, \n",
    "    \"You are a sentiment classifier.\", \n",
    "    zero_shot_prompt,\n",
    "    temperature=0.1\n",
    ")\n",
    "response_zero = output_zero.strip().split(\"</think>\")[-1].strip()\n",
    "print(f\"Response: {response_zero}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Few-Shot Example: Examples provided in the prompt\n",
    "# ============================================================================\n",
    "few_shot_prompt = \"\"\"Classify the sentiment of the following review as positive, negative, or neutral.\n",
    "\n",
    "Examples:\n",
    "Review: I love this product! It works perfectly.\n",
    "Sentiment: positive\n",
    "\n",
    "Review: This is terrible quality, very disappointed.\n",
    "Sentiment: negative\n",
    "\n",
    "Review: The product is okay, nothing special.\n",
    "Sentiment: neutral\n",
    "\n",
    "Now classify this review:\n",
    "Review: The product arrived damaged and the customer service was unhelpful.\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "print(\"=== FEW-SHOT ===\")\n",
    "output_few = generate_response(\n",
    "    deepseek_model_client,\n",
    "    \"You are a sentiment classifier.\",\n",
    "    few_shot_prompt,\n",
    "    temperature=0.1\n",
    ")\n",
    "response_few = output_few.strip().split(\"</think>\")[-1].strip()\n",
    "print(f\"Response: {response_few}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Chain-of-Thought (CoT) Prompting\n",
    "\n",
    "Encourages step-by-step reasoning for complex problems.\n",
    "CoT improves accuracy on multi-step reasoning tasks by making intermediate steps explicit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STANDARD PROMPT ===\n",
      "Response: **Solution:**\n",
      "\n",
      "1. **Initial number of apples:**  \n",
      "   The store starts with **15 apples**.\n",
      "\n",
      "2. **Apples sold during the day:**  \n",
      "   - **Morning sales:** 6 apples  \n",
      "   - **Afternoon sales:** 4 apples  \n",
      "   - **Total apples sold:** \\(6 + 4 = 10\\) apples\n",
      "\n",
      "3. **Calculating the remaining apples:**  \n",
      "   Subtract the total apples sold from the initial number of apples:  \n",
      "   \\[\n",
      "   15 \\text{ apples} - 10 \\text{ apples} = 5 \\text{ apples}\n",
      "   \\]\n",
      "\n",
      "**Final Answer:**  \n",
      "\\[\n",
      "\\boxed{5}\n",
      "\\]\n",
      "\n",
      "=== CHAIN-OF-THOUGHT PROMPT ===\n",
      "Response: Let's solve the problem step by step:\n",
      "\n",
      "**Problem Statement:**\n",
      "A store has 15 apples. They sell 6 apples in the morning and 4 apples in the afternoon. How many apples are left?\n",
      "\n",
      "**Solution:**\n",
      "\n",
      "1. **Initial Number of Apples:**\n",
      "   \\[\n",
      "   \\text{Total apples} = 15\n",
      "   \\]\n",
      "\n",
      "2. **Apples Sold in the Morning:**\n",
      "   \\[\n",
      "   \\text{Apples sold in the morning} = 6\n",
      "   \\]\n",
      "   \n",
      "   **Apples Remaining After Morning Sales:**\n",
      "   \\[\n",
      "   15 - 6 = 9\n",
      "   \\]\n",
      "\n",
      "3. **Apples Sold in the Afternoon:**\n",
      "   \\[\n",
      "   \\text{Apples sold in the afternoon} = 4\n",
      "   \\]\n",
      "   \n",
      "   **Apples Remaining After Afternoon Sales:**\n",
      "   \\[\n",
      "   9 - 4 = 5\n",
      "   \\]\n",
      "\n",
      "4. **Final Answer:**\n",
      "   \\[\n",
      "   \\boxed{5}\n",
      "   \\]\n",
      "\n",
      "**Conclusion:**\n",
      "After selling apples in the morning and afternoon, there are \\(\\boxed{5}\\) apples left in the store.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Chain-of-Thought (CoT) Prompting: Standard vs. CoT Comparison\n",
    "\n",
    "This example compares:\n",
    "1. Standard Prompt: Direct question without explicit reasoning steps\n",
    "2. Chain-of-Thought Prompt: Explicitly requests step-by-step reasoning\n",
    "\n",
    "CoT is particularly effective for:\n",
    "- Multi-step math problems\n",
    "- Logical reasoning tasks\n",
    "- Complex problem-solving\n",
    "\"\"\"\n",
    "\n",
    "# Math problem to solve\n",
    "math_problem = \"A store has 15 apples. They sell 6 apples in the morning and 4 apples in the afternoon. How many apples are left?\"\n",
    "\n",
    "# ============================================================================\n",
    "# Standard Prompt: No explicit reasoning steps requested\n",
    "# ============================================================================\n",
    "standard_prompt = f\"\"\"Solve this math problem:\n",
    "{math_problem}\"\"\"\n",
    "\n",
    "print(\"=== STANDARD PROMPT ===\")\n",
    "output_standard = generate_response(\n",
    "    deepseek_model_client,\n",
    "    \"You are a math problem solver.\",\n",
    "    standard_prompt,\n",
    "    temperature=0.1\n",
    ")\n",
    "response_standard = output_standard.strip().split(\"</think>\")[-1].strip()\n",
    "print(f\"Response: {response_standard}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Chain-of-Thought Prompt: Explicitly requests step-by-step reasoning\n",
    "# ============================================================================\n",
    "cot_prompt = f\"\"\"Solve this math problem step by step:\n",
    "{math_problem}\n",
    "\n",
    "Let's think through this step by step:\n",
    "1. Start with the initial number of apples\n",
    "2. Subtract the apples sold in the morning\n",
    "3. Subtract the apples sold in the afternoon\n",
    "4. State the final answer\"\"\"\n",
    "\n",
    "print(\"=== CHAIN-OF-THOUGHT PROMPT ===\")\n",
    "output_cot = generate_response(\n",
    "    deepseek_model_client,\n",
    "    \"You are a math problem solver. Show your reasoning step by step.\",\n",
    "    cot_prompt,\n",
    "    temperature=0.1\n",
    ")\n",
    "response_cot = output_cot.strip().split(\"</think>\")[-1].strip()\n",
    "print(f\"Response: {response_cot}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìã Output Format Control (Structured Outputs)\n",
    "\n",
    "Demonstrates how to generate structured outputs (JSON, formatted text) by\n",
    "explicitly instructing the model on the desired format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== JSON FORMAT OUTPUT ===\n",
      "```json\n",
      "{\n",
      "  \"name\": \"John Smith\",\n",
      "  \"age\": 35,\n",
      "  \"occupation\": \"software engineer\",\n",
      "  \"company\": \"Google\",\n",
      "  \"location\": \"Mountain View, California\"\n",
      "}\n",
      "```\n",
      "\n",
      "‚úì Valid JSON parsed successfully!\n",
      "Extracted data: {'name': 'John Smith', 'age': 35, 'occupation': 'software engineer', 'company': 'Google', 'location': 'Mountain View, California'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Output Format Control: JSON Structured Output\n",
    "\n",
    "Demonstrates how to generate structured JSON outputs by:\n",
    "1. Explicitly specifying the JSON schema in the prompt\n",
    "2. Instructing the model to return valid JSON\n",
    "3. Parsing and validating the output\n",
    "\n",
    "This technique enables programmatic extraction of information from LLM responses.\n",
    "\"\"\"\n",
    "\n",
    "# JSON extraction prompt with explicit schema\n",
    "json_prompt = \"\"\"Extract information from the following text and return it as JSON:\n",
    "Text: \"John Smith is a 35-year-old software engineer working at Google in Mountain View, California.\"\n",
    "\n",
    "Return the information in this JSON format:\n",
    "{\n",
    "  \"name\": \"...\",\n",
    "  \"age\": ...,\n",
    "  \"occupation\": \"...\",\n",
    "  \"company\": \"...\",\n",
    "  \"location\": \"...\"\n",
    "}\"\"\"\n",
    "\n",
    "print(\"=== JSON FORMAT OUTPUT ===\")\n",
    "output_json = generate_response(\n",
    "    deepseek_model_client,\n",
    "    \"You are a data extraction assistant. Always return valid JSON.\",\n",
    "    json_prompt,\n",
    "    temperature=0.1  # Low temperature for consistent format\n",
    ")\n",
    "response_json = output_json.strip().split(\"</think>\")[-1].strip()\n",
    "print(response_json)\n",
    "\n",
    "# ============================================================================\n",
    "# Validate JSON Format\n",
    "# ============================================================================\n",
    "import json\n",
    "\n",
    "try:\n",
    "    # Extract JSON from response if it's wrapped in markdown code blocks\n",
    "    json_str = response_json\n",
    "    \n",
    "    # Handle markdown code block formatting (```json ... ```)\n",
    "    if \"```json\" in json_str:\n",
    "        json_str = json_str.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "    elif \"```\" in json_str:\n",
    "        # Handle generic code blocks\n",
    "        json_str = json_str.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "    \n",
    "    # Parse JSON to verify it's valid\n",
    "    parsed = json.loads(json_str)\n",
    "    print(\"\\n‚úì Valid JSON parsed successfully!\")\n",
    "    print(f\"Extracted data: {parsed}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚úó JSON parsing failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Prompt Templates and Best Practices\n",
    "\n",
    "Demonstrates different prompt template patterns for consistent, reusable prompts.\n",
    "Templates help standardize prompt structure across different tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ROLE-BASED TEMPLATE ===\n",
      "Template: Role: expert data analyst\n",
      "Task: analyze the sentiment\n",
      "Input: The new product launch was a huge succe...\n",
      "Response: The sentiment of the sentence \"The new product launch was a huge success!\" is positive. The use of the word \"success\" and the emphasis with \"huge\" clearly indicate a positive sentiment....\n",
      "\n",
      "=== INSTRUCTION-BASED TEMPLATE ===\n",
      "Template: Instructions: Provide a concise summary in 2-3 sentences\n",
      "\n",
      "Context: AI is transforming healthcare thr...\n",
      "Response: The main impact of AI in healthcare is revolutionizing diagnostics and personalizing treatment plans. AI enhances the accuracy and speed of disease detection through advanced diagnostic tools and tail...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Prompt Templates: Reusable Prompt Patterns\n",
    "\n",
    "This example demonstrates three common prompt template patterns:\n",
    "1. Role-Based: Emphasizes the model's role/persona\n",
    "2. Instruction-Based: Separates instructions, context, and question\n",
    "3. Conversational: Natural conversation flow\n",
    "\n",
    "Templates help:\n",
    "- Standardize prompt structure\n",
    "- Improve consistency\n",
    "- Enable easy customization\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# Template 1: Role-Based Template\n",
    "# ============================================================================\n",
    "# Emphasizes the model's role and task\n",
    "role_template = \"\"\"Role: {role}\n",
    "Task: {task}\n",
    "Input: {input}\n",
    "Output:\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# Template 2: Instruction-Based Template\n",
    "# ============================================================================\n",
    "# Separates instructions, context, and question for clarity\n",
    "instruction_template = \"\"\"Instructions: {instructions}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# Template 3: Conversational Template\n",
    "# ============================================================================\n",
    "# Natural conversation flow\n",
    "conversational_template = \"\"\"You are {role}. The user asks: {question}\n",
    "\n",
    "Your response:\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# Example Usage: Role-Based Template\n",
    "# ============================================================================\n",
    "print(\"=== ROLE-BASED TEMPLATE ===\")\n",
    "prompt1 = role_template.format(\n",
    "    role=\"expert data analyst\",\n",
    "    task=\"analyze the sentiment\",\n",
    "    input=\"The new product launch was a huge success!\"\n",
    ")\n",
    "output1 = generate_response(deepseek_model_client, \"\", prompt1, temperature=0.1)\n",
    "print(f\"Template: {prompt1[:100]}...\")\n",
    "print(f\"Response: {output1.strip().split('</think>')[-1].strip()[:200]}...\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Example Usage: Instruction-Based Template\n",
    "# ============================================================================\n",
    "print(\"=== INSTRUCTION-BASED TEMPLATE ===\")\n",
    "prompt2 = instruction_template.format(\n",
    "    instructions=\"Provide a concise summary in 2-3 sentences\",\n",
    "    context=\"AI is transforming healthcare through diagnostic tools and personalized treatment plans.\",\n",
    "    question=\"What is the main impact of AI in healthcare?\"\n",
    ")\n",
    "output2 = generate_response(deepseek_model_client, \"\", prompt2, temperature=0.1)\n",
    "print(f\"Template: {prompt2[:100]}...\")\n",
    "print(f\"Response: {output2.strip().split('</think>')[-1].strip()[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîó Prompt Chaining (Multi-Step Reasoning)\n",
    "\n",
    "Demonstrates multi-step workflows where each step builds on previous outputs.\n",
    "Useful for complex tasks requiring decomposition (research, analysis, summarization).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 1: EXTRACTION ===\n",
      "Extracted topics: The main topics extracted from the text are:\n",
      "\n",
      "1. Machine learning models\n",
      "2. Deep learning\n",
      "3. Computational resources\n",
      "\n",
      "=== STEP 2: ANALYSIS ===\n",
      "Analysis: The relationship between machine learning models, deep learning, and computational resources is intricate and mutually reinforcing. Here's a structured summary of the analysis:\n",
      "\n",
      "1. **Machine Learning Models and Deep Learning**:\n",
      "   - Machine learning models encompass a range of algorithms, from simple linear regression to complex deep learning models.\n",
      "   - Deep learning is a subset of machine learning that utilizes neural networks with multiple layers, enabling the handling of complex tasks such as image and speech recognition.\n",
      "\n",
      "2. **Role of Computational Resources**:\n",
      "   - Computational resources, including CPUs, GPUs, memory, and cloud services, are crucial for training and running machine learning models, especially deep learning ones.\n",
      "   - The complexity of deep learning models necessitates significant computational power, often provided by GPUs, which accelerate training processes.\n",
      "\n",
      "3. **Interdependence and Trade-offs**:\n",
      "   - The demand for computational resources is driven by the complexity of machine learning models. Simpler models require fewer resources, while deep learning models demand substantial computational power.\n",
      "   - There is a trade-off between model complexity and resource usage. Advanced algorithms and techniques can enhance efficiency, reducing resource requirements.\n",
      "\n",
      "4. **Future Implications**:\n",
      "   - Advancements in computational resources are expected to facilitate the development of even more complex models, potentially leading to breakthroughs in AI applications.\n",
      "   - However, the environmental impact of resource-intensive training processes must be considered, highlighting the need for sustainable practices.\n",
      "\n",
      "In conclusion, machine learning models, particularly deep learning, are highly dependent on computational resources. The availability and improvement of these resources enable the creation of sophisticated models, which in turn drive the demand for more resources, creating a cyclical and mutually reinforcing relationship.\n",
      "\n",
      "=== STEP 3: SUMMARIZATION ===\n",
      "Final Summary: Machine learning models, particularly deep learning, rely on computational resources, which enable more complex models. There's a trade-off between model complexity and resource use, with efficiency reducing demands. Advancing resources will drive AI progress, but sustainability is key to manage environmental impact.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Prompt Chaining: Multi-Step Reasoning Workflow\n",
    "\n",
    "This example demonstrates a 3-step pipeline:\n",
    "1. Extraction: Extract key topics from text\n",
    "2. Analysis: Analyze relationships between topics\n",
    "3. Summarization: Generate final summary\n",
    "\n",
    "Each step uses the output from the previous step, creating a reasoning chain.\n",
    "This technique is useful for complex tasks that benefit from decomposition.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Extract Key Information\n",
    "# ============================================================================\n",
    "step1_prompt = \"\"\"Extract the main topics from this text:\n",
    "Text: \"Machine learning models require large datasets for training. Deep learning, a subset of ML, uses neural networks. Both require significant computational resources.\"\n",
    "\n",
    "List the main topics:\"\"\"\n",
    "\n",
    "print(\"=== STEP 1: EXTRACTION ===\")\n",
    "step1_output = generate_response(\n",
    "    deepseek_model_client,\n",
    "    \"You are an information extraction assistant.\",\n",
    "    step1_prompt,\n",
    "    temperature=0.1\n",
    ")\n",
    "step1_result = step1_output.strip().split(\"</think>\")[-1].strip()\n",
    "print(f\"Extracted topics: {step1_result}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Analyze Extracted Information\n",
    "# ============================================================================\n",
    "# Uses output from Step 1 as input\n",
    "step2_prompt = f\"\"\"Based on these topics: {step1_result}\n",
    "\n",
    "Analyze the relationship between these topics and provide insights.\"\"\"\n",
    "\n",
    "print(\"=== STEP 2: ANALYSIS ===\")\n",
    "step2_output = generate_response(\n",
    "    deepseek_model_client,\n",
    "    \"You are an analytical assistant.\",\n",
    "    step2_prompt,\n",
    "    temperature=0.1\n",
    ")\n",
    "step2_result = step2_output.strip().split(\"</think>\")[-1].strip()\n",
    "print(f\"Analysis: {step2_result}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Generate Final Summary\n",
    "# ============================================================================\n",
    "# Uses outputs from both Step 1 and Step 2\n",
    "step3_prompt = f\"\"\"Based on the topics and analysis:\n",
    "Topics: {step1_result}\n",
    "Analysis: {step2_result}\n",
    "\n",
    "Generate a concise summary (2-3 sentences):\"\"\"\n",
    "\n",
    "print(\"=== STEP 3: SUMMARIZATION ===\")\n",
    "step3_output = generate_response(\n",
    "    deepseek_model_client,\n",
    "    \"You are a summarization assistant.\",\n",
    "    step3_prompt,\n",
    "    temperature=0.1\n",
    ")\n",
    "step3_result = step3_output.strip().split(\"</think>\")[-1].strip()\n",
    "print(f\"Final Summary: {step3_result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Prompt Performance Visualization\n",
    "\n",
    "Quantitative comparison of different prompt variations across metrics:\n",
    "- Response length (verbosity)\n",
    "- Response time (latency)\n",
    "- Accuracy (with labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROMPT PERFORMANCE COMPARISON ===\n",
      "  Prompt Type  Response Length  Time (s)                                                                                                  Response\n",
      "        Basic               31      3.80                                                                           The capital of France is Paris.\n",
      " With Context              448      6.99   The capital of France is Paris. This conclusion is based on the recognition of Paris as the politica...\n",
      "With Examples               31     14.84                                                                           The capital of France is Paris.\n",
      "          CoT              996     14.44 The capital of France is Paris. This conclusion is based on several key points:\\n\\n1. **Iconic Landmar...\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAHqCAYAAADVi/1VAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAbY1JREFUeJzt3Qm8TfX+//HPQcYIx3CpqDSYh8hQVDSSkKHJ1aAbhaThEpWKZCpNhlIZcioSdZHUbVJdogiVuFSGMs8zOWf/H+/v76793+c4OIdz1t5nr9fz8dgP9trDWXuttff+7M/6fD/fhFAoFDIAAAAAAADAR7n8/GMAAAAAAACAkJQCAAAAAACA70hKAQAAAAAAwHckpQAAAAAAAOA7klIAAAAAAADwHUkpAAAAAAAA+I6kFAAAAAAAAHxHUgoAAAAAAAC+IykFABFCoRDbwwdsZwAAEFTEQcD/lyfi/wAyqUOHDjZ//vxUyxISEqxgwYJ21lln2e23324tW7Zku6bj5ZdftuHDh9vy5ctjZvusWLHCHn/8cZs4cWJ42QUXXGDdunWz++67L8PP88cff9gVV1xhAwcOtNatW2f7MXfKKadYiRIlrHHjxtajRw877bTTLJalt50jTZ061Xr37n3c54mlYwcAkHHETyfmkUcesffff/+Y96lbt67dcMMN7nv0s88+szPOOCMq+1cmTJiQpc+bXnyQK1cuO/XUU61atWrWtWtXq127tsWyQ4cO2bPPPmtVq1a1Fi1apHsfxZ7Hkx0xJhAtJKWAk1S5cmV74oknwteTk5Ntw4YNNm7cOOvZs6cVLVrULrvsMrZzDjBr1iz74YcfLKcdc3/99Zf9/PPPNmzYMPvll1/snXfeccnRnLqdL7/8cps0aVL4+pdffmmjRo1yScySJUv6tJYAgOxE/JR5Xbp0sZtvvjl8feTIkbZ06VL3/ehRgqZ48eLue7RUqVIWjyLjgZSUFNuyZYuNGDHCnQx+7733rGLFiharNm3aZOPHj3dJpaOJjIHkpptusrZt21q7du3Cy8qVK5et6wn4iaQUcJL05V+zZs0jll966aXWoEEDd1aHpBSy+5i76KKLbO/evfbSSy/Z4sWL0z0mcwoF07p4fvvtN/dvpUqVonLGFwCQ9YifMk+JiMhkhL4r8+bNm+53fuT3aLxJLx5QkvOqq66yt99+2/r162c5WXr7829/+1uOju2AY6GnFJBN8uXL5wKFyIoVnc0ZPXq0+9JU2e4111xzRGnzmjVr7J577rF69epZjRo13NmR2bNnpxr21qRJE/viiy/s2muvdfe58cYbbd68eUeciVGJsxJi1atXd2dYVMadtjz4rbfeskcffdSVe9eqVcvuv/9+d8Ypo+sj//3vf61z58524YUXuovKp9euXZsl23HdunX24IMPuvXT39dZMJ0VjBwqp9fx0UcfWffu3d1r0H0fe+wx27dvX6pqIpVLK1mo7XHXXXfZBx984B6r5/CGE3rbRdc9e/bsSbWN9Hcit9HRbNy40W0X/T3tByWMVEkngwcPdst3796d6jE666nS8/3792d6W+mY8raZVz7/8MMPu/VVIHPnnXe65fqbOkN35ZVXunL35s2buzOLkXSMaXs888wzbt/rdT/00EMu8aVjWNtR66lhjdu3b0/1uOeff949TokyPVYVgzt27HC3H2s7Z4aeT+uu6rBI2m5aL1VWeX8jKSnJevXq5V7DxRdfbAMGDLCDBw+metynn37qyuD1nJdccok9/fTTqY4fAIA/iJ9OPn7SCVEvvvGG/SnuUQWOvvsVf6ji6vfff3fx5PXXX+9iLFXiqOI60vfff29///vf3e2Kg/R9um3btgyth6qX9L2r719VeXmxoYbxa/3SVgStX7/eJZymTZuW6desJFWxYsXCMZC2gRJVkydPdt/rWveVK1e622bOnOm+87Veuq1v3762c+fO8HMpNlGM/e9//9vFSIoN1I5DVd6LFi1y20nbULfNnTs3wzG6195BFKPrvidKsZ1iMf22iKR4Vb8vvP2uWFAxnlo86PUqjl62bFmmYm0gu5GUArKgUeHhw4fDF/3YVWWHvmz0Az6yp9STTz7pEhMaQ/7KK6+4Lyz9eNeXtuiLRUkM/bAeMmSIS1Bo+N+9995rq1evDj+PggEFBbfeequ9+OKLlj9/fhdseIGEEiZKQimQeOCBB9yX5Omnn+6SRWm/6JVA0N/Vj3slD/RFqnXK6PoooFFgs3XrVpdo0Q9+BR233HKLW3Yy9Dr13Bqaph5Ezz33nFun9u3b26+//prqvhrOpteoddS20Bewl5gQBRwql1Zgpe2tHkx6To8CDG0zUZAUWSL95ptvuqSWtrUSM59//nmGzsJpuycmJrq/16ZNG7fPtY1Ef0vHioayRfrXv/5lzZo1swIFCmR6e2lfyJlnnhlepmRdoUKF3Lb4xz/+YQcOHHDHzfTp0911LwmmIEbrF2nMmDEuQNQxon0+Y8YM9zq++eYb69+/vwtglOjUMR1JZykXLlzoEl/aXkpi6jjSe+VY2zkzdBwqsNbriGwWqgBSyaRWrVqFl2m/6Vh84YUX3GvW39X7x6Pn0HvjnHPOcftKPcT0PlEATSNSAMgexE/ZFz+lRwkVnaRRokLfz4qjOnXq5P6v72jFgfrO18ksz3fffWd33HGHizP1HdqnTx/X1/K2225z8cSxLFiwwD788EMXf+lEjxIhepxO9J133nku+aGYJ5JOFqov69VXX53p16cTZLpEVpLpRKBiGW1bxeUVKlRwcY/iF52sU/yi7/+PP/7YJW8iX5NacQwaNMidmFUcsWvXLpcI0mMVuyhe0DGsODvycceK0TWc0jsxp7gqcthlZimW0snPyJPSWg/Fleop5tHfVRyn2Gbo0KFuGykW1snrzMbaQLYJAThhf//730Pnn3/+EZcLLrggdP3114c++uij8H1/++03t/zVV19N9RzPP/98qFq1aqFt27aFNm3a5B4/bdq08O27du0KPfPMM6H//ve/7vpLL73k7vP++++H77N///7QJZdcEurRo4e7PmTIkFCVKlVCf/zxR6q/dfvtt7v7JScnu+t6nltuuSXVfR555JFQzZo13f8zsj4PPvhg6OKLLw7t3r07fJ/t27eHateuHRo0aNBRt533Oo5l2LBhbttEvo6DBw+GrrjiitB9993nrq9du9Y9z8MPP5zqsR06dAg1b97c/X/16tVu248ZMybVfTp27Ogeq+c42jrpert27VIt09+66KKLjrre3jp16tQp1fIBAwa4/aLtIzfddFOoffv24dsXLFjgHrdw4cJjHnN6zF9//RW+bNmyJTRz5sxQ3bp13XOmpKSE71ujRg23zTxvvfVWun+jT58+blt769a4ceNQo0aN3PN7rr322lCtWrXcMeDp3LlzqEWLFuHrepzWI/I+//73v93fnD179lG387FMmTIl1X7yfP3112753Llzw8vuvPNOt189uv3qq69O9TrGjh3rlq9cudJtq0svvTR01113pXruOXPmuPt88cUXGV5PAEDGED+dePwUqVevXu5793jfm7qf973n6du3r1um7zvPG2+84Zbt3LnTXVdMoVjq8OHDqeLZSpUqhZKSko65f6tWrRpav359eNnSpUvdc0+YMMFdnzhxoovN1qxZE76Pvq8ff/zxoz6v97oU13kx0J49e0I//fRT6I477ghVrlw5tGzZslT3/eCDD8KP37Fjh1uvtH/ju+++c/f1XpMXp3hxiyh+17LJkyeHl82aNcst02vLaIzuxYhav4zS/fXckRTLK37p2bNneJni9YoVK4a3u7ff9fo8GzdudPHe0KFDMxxrA9mNSingJFWpUsVV5eiisy/nn3++m3lPZ5RUCeX59ttv3RkVlepGVlbpuipmdEZJ1TvnnnuuO1Ohsyyq4NDZCp3d0VklT548eVzJsEdnYVTCqzNaorNYKtFV5VAkVWht3rw53KNH0o5P15h1b+hYRtZHr0vlvloH7zWpT0SdOnVszpw5J7VtVRKtMu7SpUuHn1uzrOi1pn3u9F6HN/xKZ5G07SP3h0Ruw2NJO5OLSsR1xux4mjZtmuq6zvyp4ko9n0RVR6pm+/PPP911zahz9tlnu313LNrPOu68i0rjdeZOw/d0hityyKiqfzSM1KNjQ8dF2r+hY0PHobduotJ0HWseHQ9av8KFC6eqWEo7BFHHdOR9dF3P4x2fWUWvu2zZsuEzrTqrqWMm8gyhaFhC5Ovwytq1Pnov6HFp35caeqjj+D//+U+WrjMA4P8QP2Vf/JQezcyrSqHI73RRxVLkd7ooxlEsqJhA7Qciq9pUja3nOd73o9o5KBbzKJ7TY71Y4LrrrnOv3fsOV4X1qlWrjvgOT4/aYHgxkP6OhuKpgl+VQGlnrtPf9WjonWa/Sxv/aZsrNko7u7GeO6PbK6MxelZRPKxt9cknn4TjdsWRio0it7tiVr0+j6q1FAN665OZWBvILjQ6B06ShkZprLlHX1b6gd+xY0c3nt1rNOn11NGXcHpUgqtkgsqMNdRKw5BUxnzKKae4YUpPPfWUCyi8L8bIH9miYWLe39C4+MghXGm/UCO/PNMOE9MXkTdkKSPro7+psfm6ZHWTTT23ggwFHemJ7Lt0rNfh9T7QNoqU9vrRqJT8aM99LGlnivO2h9e3QMP0NFRSAZlKuzXUTqX0x6Ptoe3v7SP13yhTpowLZtM7PiPpb6c3g116x0Z6z5d2W6RHgU3a7aU+D5H9GrKCnleB6NixY93wTW1HrbOC1WOtj7fftT7ee0bb09umkbzydgBA1iJ+yr74KT3pfacf63td8YBORL722mvukpZij2Px4oq0379enKH10clCDZfX0DLFmBk5MSeKS71YRnGpYoy03/XpvT4vDklv3bQs7Um29LbZ8dorHC9Gz0peawglpurXr+8STOqfGim97aL10XC9jMbaJ9JSAsgMklJAFtOXkcbPq2G4xrCrckWKFCni/lVfo7SJAlHFh/flod5T+pGt8fcaG65gQF+4WibpfbGpj5T3Y1vJIlVEpeUt03Nl1PHWRxUxOivjNdGOlPZLObP03DqLqF5X6YmsADrea/C2kbedJaONOk9U2iSM1xzd2086DhSQKRmlCjtVdkX2IMtoIJ8ZOjYi+5OdzLFxNJGNz72eDlqWHUG2klLq6/DVV1+57ahEX9pAOe36ePtB6+O9L3WM6VhLy0sEAwCyF/FT1sVPWUGxhk58qadUeidUj5eoSO9ElGKNyKSTkiqq7lmyZInr66QTdBmhmOlEZuP1vtMVB6iSPO26pXdCN7OOF6NnJa2vYhfFP/q7SqLpxPGxYqC065NVsTZwMhi+B2QDJRoaNWrkGkN7pcBe6ay+HJRQ8C5KjKgRor5M1IRSCR59OSsQUDmtGijqy9ebTcRrZPj111+nuq4f5Q0aNHDXNfRIz+UNC/PobJTOLJUvXz5DryMj6+PNZqLbvNekYWTjxo1z1VUnQ8+t5t06cxa5zVQRo+GSuXPnzvDwO9037frozFLaypus9OWXX6a6roafCuIiS7/VqFKzFypZqW19tDN9WUXHho4L7du0x4bONmrI3snSsajyeI+aoasc3Ds+s3I7q9xez6tm9GrmqSRVWmpMH0mBr45nnVVUUKrATDPiRB5j2g9KKDP7DAD4h/gpa+KnrKAEh2av0zD3yO9HtW/QRC5pZ31OS20pIiuPNBRQ8Ye+eyNjErW80LA73TcjJ+ZOhuIvJVkUn0dSKwXFtZHD9U7U8WL0jMauGaU4UsPs9JrSOzGnIZGRDcs1MkMxoLc+WRVrAycj+ml4IE5phhIN49OMIzoLpDHuuq7+TPpSVuJGXwKaEUNne/SlrB/uGnuusxX33XefO2uoLxr92NaMJZHU16lHjx7uB/Ubb7zhqmw0k4eoaklJBp3dUkm0xryrLFr9nzRcLKNJAQUjx1sfzVCmWTs0c4tmjNGXoWY3+/TTT4+YlS09Cr7SUvWKkgtaf30p6l8Nh1QVj8rc3333Xff6M3MmSWfjNLOMejpVrFjRBXyaaVC87eFVzeiLXYHLyZ4xU9JLyQ0lmzRjnbaLKugiy8GVMFMgoOSljoXspu2q2fE024xmkdGxp6TNlClT3LHibYOTodl7dCzqGNH/td2VpK1Xr162bGcFZOqppR4XkQm/yB4Smk1Iwa6q/RRMa4pm7+8q0arqRgVemjJZQwvUH06B29HK2QEA2YP4KWPxkx/03aq2AppJVzGsN5udEkyK/45FQ//0WM1epxOyOtGjk5p6nkiKz3Sbehhl94k5xcNaJ1VY60ScvvN1Ukonh9VDNSP9rDLiWDG613NTQ+2OFrdkhvpkakZknUCOnFXao3YT2geKdRTnaMY/VYxptkHJqlgbOBkkpYBsogoMfeDry/udd95x069q2t1XX33VJk6c6Jor68tKZzX0xaUvCl10f305a+iffhwrWdWvX78jKkA0pE4JJlVa6cyO/oZXAaVqKF3X8ygp5iVi9EP7iiuuyPBrUILpeOuj533rrbdcQkXJK335KejQF35G/pa2SVqazlfPr+BE20p/X69Xjbj197UuSkRkhr6o1VdAr0fTEesMkQIErafXb0CNyPXFrOmS9fz6myfj0UcfddVRSrxpnyjQTptclMsvv9ztx7Ql19lBlVoTJkxw21RBmLaFjtUT2aZHozJ/JZ50XGvbKshTMOTJ6u2sJqyqfEqvSkpuv/12l2BS0k3BloIzJVE9mtpZwxRef/11lzjUOus9pb4MWVHKDwDIOOKnjMVPfmjYsKFLqiiRoRNZSuToZI16OaadYCYtxTRqmfDPf/7TnXRVAkhxUdpKHn2HKyY52nd4VvNOsiYlJbnvfCWqVKHnxSxZ4Vgxuk5M6uSx/vbs2bNdw3ht1xOl7anqM1W0pVftrn2gZJPWR/2hdKJUPbm8Ju1ZGWsDJypBU/Cd8KMB+E5VHgoOli9fztbPIA2NVOm0qnUieyYNHjzYNaM/Xgl6dtJHsJI4CvyUtMrpNIudSsEHDRrk29/UGT0lRBXcpe3ZoApFJaMUhAIAgov4KTaNHj3anbxTy4Oc3r8oGseYhgcqsafKNZ2Ei6STf6rET9vGAIg1VEoBiHuqDtIZH/W90he2zoRpSJfOkkVWzPhJFUoKwn788Udbu3ZtuIwaGachotp+OsOnM6zZ0UQUAABkPbW2UE9NtRRQQiWnJ6T8plYg2oZqq6FqcQ2DBHIqklIA4p5Km5UAeuGFF9xZI5Uva4hgr169rH379lFZJ/XqUjJFPRdUUs0wscxTHwg1iFdfLg0PAAAAOYN6PCoOuuqqq9zwMmSO+qGqHYPaD6iFRmS/UiCnYfgeAAAAAAAAfJe1858DAAAAAAAAGUBSCgAAAAAAAL4jKQUAAAAAAADfkZQCAAAAAACA70hKAQAAAAAAwHd5/P+TOcfWrbstFIr2WsSuhASzxMTCbKeAYb8HE/s9eNjnmdtOQUOMdGy8f4KJ/R487PNgYr9nbYxEUuoYlJAiKXV8bKdgYr8HE/s9eNjn4Ljg/QM+N8F3JYiRsgvD9wAAAAAAAOA7klIAAAAAAADwHUkpAAAAAAAA+I6kFAAAAAAAAHxHUgoAAAAAAAC+IykFAAAAAAAA35GUAgAAAAAAgO9ISgEAAAAAACCYSalDhw5Z8+bNbd68eeFla9eutTvuuMNq1qxpzZo1s2+++SbVY+bMmeMeU6NGDbvtttvc/SONGzfOGjVqZLVq1bI+ffrY/v37fXs9AAAAAAAAiPGk1MGDB+3BBx+0FStWhJeFQiHr2rWrlShRwqZMmWItW7a0bt262bp169zt+le3t27d2t577z0rXry4denSxT1OPv74Yxs+fLj169fPxo8fb4sXL7ahQ4dG7TUCAAAAAAAghpJSK1eutBtvvNHWrFmTavm3337rKp+UVKpQoYJ17tzZVUwpQSWTJ0+2qlWrWseOHe28886zgQMH2p9//mnz5893t7/55pt2++23W+PGja169er21FNPucdSLQUAAAAAABAbopqUUhKpXr16NmnSpFTLVdlUuXJlK1iwYHhZ7dq1bdGiReHb69SpE76tQIECVqVKFXd7cnKy/fjjj6luV0Lrr7/+smXLlvnyugAAAAAAAHBseSyKbr311nSXb9682UqVKpVqWWJiom3YsOG4t+/atcsNCYy8PU+ePFa0aNHw4zMqISFTdw8cb/uwnYKF/R5M7PfgYZ9nbjsBAAAghyWljkbD7PLmzZtqma6rIfrxbj9w4ED4+tEen1GJiYVP8BUEC9spmNjvwcR+Dx72OQAAAAKVlMqXL5/t2LEj1TIllPLnzx++PW2CSdeLFCnibvOup71dw/wyY+vW3fa/3uk4ytlh/VhhOwUL+z2Y2O+ZkytXgiXk8BIarX7RooVsx469Of67UBOhpKSEsvW9AQDI/HelLvEgd+6ozx+WJfRdmV3fl0COSkqVLl3aNUGPtGXLlvCQPN2u62lvr1Spkhump8SUrqtJuhw+fNgluUqWLJmp9VAQntMDcT+wnYKJ/R5M7PfjU4CtZE68BNp6LTmdAuzt2/cSaANAjNB3ZPGiBS0hTpI5xYrl/O9KCSWn2LYd+/i+hK9iMilVo0YNGz16tBuK51VHLViwwDU7927XdY+G8y1dutS6detmuXLlsmrVqrnb1URd1ABdfaUqVqwYpVcEAAjamd9pq3bb1gOHo706gZeYP4+1OKuw2yec/QWAGKoozp3L9k2dasmbN0d7daBqr5IlrWDr1nxfwncxmZSqW7eulSlTxnr37m1dunSxL774wpYsWWIDBw50t7dp08beeOMNl7hq3LixjRgxws4444xwEkoN1Pv27Wvnn3++q6568skn7cYbb8z08D0AAE6UElIb9yezAQEAOAolpFIyORkVgPgSk/WSuXPntpEjR7pZ9lq3bm3Tpk1ziaeyZcu625WAevnll23KlCnWtm1bNzRPt3v9O6677jrr3LmzS0x17NjRqlevbv/85z+j/KoAAAAAAAAQc5VSy5cvT3W9fPnylpSUdNT7X3bZZe5yNJ06dXIXAAAAAAAAxJ6YrJQCAAAAAABAfCMpBQAAAAAAAN+RlAIAAAAAAEBwe0oBAAAAAID4lStXgrvEg9y5c36NT0pKyF2iiaQUAAAAAADIVkpGFS9a0BLiIJkjxYoVspwulJxi23bsi2piiqQUAAAAAADI9qSUElL7pk615M2b2dpRlrtkSSvYurXbLySlAAAAAABA3FNCKmXDhmivBmJEfNTNAQAAAAAAIEchKQUAAAAAAADfkZQCAACIQ4cOHbLmzZvbvHnzjrht9+7d1qhRI5s6dWpU1g0AAEBISgEAAMSZgwcP2oMPPmgrVqxI9/ahQ4fapk2bfF8vAACASCSlAAAA4sjKlSvtxhtvtDVr1qR7+/fff2/ffvutlSxZ0vd1AwAAiERSCgAAII7Mnz/f6tWrZ5MmTUp3SN/jjz9uffv2tbx580Zl/QAAADx5wv8DAABAjnfrrbce9bZXXnnFKleubA0bNjypv5GQcFIPj3ve9mE7BQv7HfGCz67gSUiI3nOSlAIAAAjIsL6JEyfatGnTTvq5EhMLZ8k6xTu2UzCx35GTFStWKNqrgIDtc5JSAAAAcS4UCtljjz1m3bt3txIlSpz0823duttCoSxZtbiks8NKTLCdgoX9nnG5c+eK+g9hpG/79r2WnJySLZuH/R6sfZ7wv+/C4yEpBQAAEOfWrVtnP/zwgy1fvtwGDx7slu3fv9+eeOIJmzlzpr3++uuZej4lpEhKsZ3A+wPxic/34AlF8UQTSSkAAIA4V7p0afvkk09SLevQoYO7tGjRImrrBQAAgo2kFAAAQJzLkyePlS9f/ohliYmJLmEFAAAQDbmi8lcBAAAAAAAQaFRKAQAAxCn1kDqazz//3Nd1AQAASItKKQAAAAAAAPiOpBQAAAAAAAB8R1IKAAAAAAAAviMpBQAAAAAAAN+RlAIAAAAAAIDvSEoBAAAAAADAdySlAAAAAAAA4DuSUgAAAAAAAPAdSSkAAAAAAAD4jqQUAAAAAAAAfEdSCgAAAAAAAL4jKQUAAAAAAADfkZQCAAAAAACA70hKAQAAAAAAwHckpQAAAAAAAOA7klIAAAAAAADwHUkpAAAAAAAA+I6kFAAAAAAAAHxHUgoAAAAAAAC+IykFAAAAAAAA35GUAgAAAAAAgO9ISgEAAAAAAMB3JKUAAAAAAADgO5JSAAAAAAAA8B1JKQAAAAAAAPiOpBQAAAAAAAB8R1IKAAAAAAAAviMpBQAAAAAAAN+RlAIAAAAAAIDvSEoBAAAAAADAdySlAAAAAAAA4DuSUgAAAAAAAPAdSSkAAAAAAAD4jqQUAAAAAAAAfEdSCgAAAAAAAL4jKQUAABCHDh06ZM2bN7d58+aFly1atMhuvvlmq1Wrll1zzTU2efLkqK4jAAAINpJSAAAAcebgwYP24IMP2ooVK8LLNm/ebHfffbfVrVvX3n//fevevbv179/fvvzyy6iuKwAACK6YTkqtX7/eOnfubBdeeKE1adLExo0bF75t6dKl1q5dO6tRo4a1adPGfvrpp1SPnTFjhl155ZXu9q5du9q2bdui8AoAAAD8tXLlSrvxxhttzZo1qZZ/+umnVqJECZesOuuss+y6666zVq1a2fTp09lFAAAgKmI6KdWjRw8rWLCgTZ061fr06WMvvPCC/fvf/7Z9+/ZZp06drE6dOu42laAreaXlsmTJEnv00UetW7duNmnSJNu1a5f17t072i8HAAAg282fP9/q1avnYqBIjRo1soEDBx5x/z179rBXAABAVOSxGLVz507X90Bl5Tqbp4uCqblz57rb8uXLZz179rSEhASXgPrqq69s1qxZ1rp1a0tKSrKmTZu6s38yZMgQa9y4sa1du9bOPPPMaL80AACAbHPrrbemu/yMM85wF8/WrVvtww8/tPvuuy/TfyMh4aRWMe5524ftFCzsd8QLPruCJyEhes8Zs0mp/PnzW4ECBVwl1EMPPeQSSgsXLnTVU4sXL7batWu7hJToXw3xUxJLSSndrp4JnjJlyljZsmXdcpJSAAAg6A4cOOCSURrOd9NNN2X68YmJhbNlveIN2ymY2O/IyYoVKxTtVUDA9nnMJqVUCdW3b19XKfXmm29acnKySzipj9Rnn31m5557bqr7JyYmhpt5btq0yUqVKnXE7Rs2bMjUOpAhztj2YTsFC/s9mNjviAfRPAsYS/bu3WtdunSxVatW2dtvv+1OAmbW1q27LRTKltWLCzoulJhgOwUL+z3jcufOFfUfwkjf9u17LTk5JVs2D/s9WPs84X/fhTk2KSW//vqrG3Z35513uoSTElQNGjSw/fv3W968eVPdV9c19bF39u9Yt2cUZznYTuD9AT4XER/48fP/+0f94x//cE3Qx48f79ojnAglpEhKsZ3A+wPxic/34AlF8URTzCal1Dvqvffes9mzZ7uhfNWqVbONGzfaqFGj3BC8tAkmXdf9vCqr9G7P7JlAzm4dG2eDgon9Hkzs94zjLGBsivZZwFiQkpLiJoH5448/bMKECVahQoVorxIAAAi4mE1K/fTTT1a+fPlwokkqV65sr7zyipt1b8uWLanur+vekL3SpUune3vJkiUztQ6cBWQ7gfcH+FxE/Aj6mV+d7Js3b547wVekSBHbvHmzW37KKadY0aJFo716AAAggHJZjFKCafXq1akqnn777Tc3a0yNGjXshx9+sND/okv9qyboWi76d8GCBeHHrV+/3l282wEAAILm448/dtVSnTt3toYNG4YvJzL7HgAAQFxXSjVp0sSGDh1qjz32mN177732+++/uyqpBx54wK699lp77rnnbMCAAXbzzTfbxIkTXZ+ppk2busfecsst1qFDB6tZs6Yb9qf7XX755cy8BwAAAmX58uXh/7/xxhtRXRcAAIAcUylVuHBhGzdunCstb9u2rQ0cONAlpzRt8amnnmqvvvqqq4bSjHyLFy+20aNHW8GCBd1ja9WqZf369bMRI0a4BNVpp53mHg8AAAAAAIDYELOVUnLuuefa2LFj072tevXq9v777x/1sUpW6QIAAAAAAIDYE7OVUgAAAAAAAIhfJKUAAAAAAADgO5JSAAAAAAAA8B1JKQAAAAAAAPiOpBQAAAAAAAB8R1IKAAAAAAAAviMpBQAAAAAAAN+RlAIAAAAAAIDvSEoBAAAAAADAdySlAAAAAAAA4DuSUgAAAAAAAPAdSSkAAAAAAAD4jqQUAAAAAAAAfEdSCgAAAAAAAL4jKQUAAAAAAADfkZQCAAAAAACA70hKAQAAAAAAwHckpQAAAAAAAOA7klIAAAAAAADwHUkpAAAAAAAA+I6kFAAAAAAAAHxHUgoAAAAAAAC+IykFAAAAAAAA35GUAgAAAAAAgO9ISgEAAAAAAMB3JKUAAAAAAADgO5JSAAAAAAAA8B1JKQAAAAAAAPiOpBQAAAAAAAB8R1IKAAAAAAAAviMpBQAAAAAAAN+RlAIAAAAAAIDvSEoBAADEoUOHDlnz5s1t3rx54WVr1661O+64w2rWrGnNmjWzb775JqrrCAAAgo2kFAAAQJw5ePCgPfjgg7ZixYrwslAoZF27drUSJUrYlClTrGXLltatWzdbt25dVNcVAAAEV55orwAAAACyzsqVK+2hhx5ySahI3377rauUmjhxohUsWNAqVKhgc+fOdQmq++67j10AAAB8R6UUAABAHJk/f77Vq1fPJk2alGr54sWLrXLlyi4h5aldu7YtWrQoCmsJAABApRQAAEBcufXWW9NdvnnzZitVqlSqZYmJibZhwwaf1gwAACA1hu8BAAAEwP79+y1v3ryplum6GqJnVkJCFq5YHPK2D9spWNjviBd8dgVPQkL0npOkFAAAQADky5fPduzYkWqZElL58+fP9HMlJhbOwjWLX2ynYGK/IycrVqxQtFcBAdvnJKUAAAACoHTp0q4JeqQtW7YcMaQvI7Zu3W1p+qgjzdlhJSbYTsHCfs+43LlzRf2HMNK3ffteS05OyZbNw34P1j5P+N934fGQlAIAAAiAGjVq2OjRo+3AgQPh6qgFCxa4ZueZpYQUSSm2E3h/ID7x+R48oSieaGL2PQAAgACoW7eulSlTxnr37m0rVqxwCaolS5ZY27Zto71qAAAgoEhKAQAABEDu3Llt5MiRbha+1q1b27Rp02zEiBFWtmzZaK8aAAAIKIbvAQAAxKnly5enul6+fHlLSkqK2voAAABEolIKAAAAAAAAviMpBQAAAAAAAN+RlAIAAAAAAIDvSEoBAAAAAADAdySlAAAAAAAA4DuSUgAAAAAAAPAdSSkAAAAAAAD4jqQUAAAAAAAAfEdSCgAAAAAAAL4jKQUAAAAAAADfkZQCAAAAAACA70hKAQAAAAAAwHckpQAAAAAAAOC7mE5KHTp0yJ566im76KKL7OKLL7Zhw4ZZKBRyty1dutTatWtnNWrUsDZt2thPP/2U6rEzZsywK6+80t3etWtX27ZtW5ReBQAAAAAAALIsKbVr1y47ePCg+/+yZcvs9ddft7lz51pWevrpp23OnDn2xhtv2HPPPWfvvvuuTZo0yfbt22edOnWyOnXq2NSpU61WrVrWuXNnt1yWLFlijz76qHXr1s3dX+vau3fvLF03AAAAAAAA+JyU+vTTT+3SSy+1BQsW2OrVq619+/b2/vvvW5cuXSwpKcmywo4dO2zKlCnWv39/q169ujVo0MA6duxoixcvtpkzZ1q+fPmsZ8+eVqFCBZeAKlSokM2aNcs9VuvQtGlTa9WqlVWsWNGGDBlis2fPtrVr12bJugEAAAAAACAKSakXXnjBunfv7obUTZ482cqUKWMffvihG143ZswYywpKeJ166qlWt27d8DJVRw0cONAlpmrXrm0JCQluuf698MILbdGiRe66blcVlUfrV7ZsWbccAAAAAAAAOTQptWbNGleJJJ999pldddVV7v/nnXdelvVuUlXT6aefbh988IFde+21dsUVV9iIESMsJSXFNm/ebKVKlUp1/8TERNuwYYP7/6ZNm455OwAAAAAAAKIrz4k8SFVH8+bNs9KlS9vvv/9uTZo0ccunT59uZ511VpasmPpDaWjgxIkTXXWUElF9+/a1AgUK2P79+y1v3ryp7q/raowuBw4cOObtGfW/QiwcZ/uwnYKF/R5M7HfEg+z4vuI7EAAAwOeklIbuqZ9TcnKyXX755VatWjUbPHiwSyANHz48a1YsTx7bs2ePa3CuiilZt26dvfPOO1a+fPkjEky6nj9/fvd/9ZtK73YltDIjMbHwSb+OIGA7BRP7PZjY78ipihUrFO1VAAAAQFYkpapWrWpfffWVbdy40SpVquSWtWvXzu666y4rUaKEZYWSJUu65JKXkJKzzz7b1q9f7/pMbdmyJdX9dd0bsqcKrvRu13Nmxtatuy0UOqmXEdd0dlg/UNlOwcJ+Dyb2e8blzp2LBEgM2r59ryUnp2TbewMAAAA+9ZS65ZZbXNWSl5CSc845J8sSUlKjRg07ePCgGx7o+e2331ySSrf98MMPFvpfxkj/Lly40C33HqtG6R4lsnTxbs8oPT2XY28DtlMwjxH2e/T3Afs9tt8fiE05YZ/v2rXLxT+ybNkye/31123u3LlZ+0cAAAByclJKyaetW7dadlKSS0MDe/fu7YKyr7/+2kaPHu0SYmp8rqBtwIABtnLlSvev+kx5zdd1n3/9619uZkA9VkMN9Vxnnnlmtq4zAADAifr000/t0ksvdSfW1Fezffv29v7771uXLl0sKSmJDQsAAOLOCQ3fq1y5sguQ1EtKlUtpm4qrMXlWePbZZ61///4uyaR+UArOOnToYAkJCfbqq6/aE088Ye+++65dcMEFLmFVsGBB97hatWpZv3797KWXXrKdO3faJZdc4p4HAAAgVr3wwguub+fFF1/sYqAyZcrYjBkz7IsvvnBxzN///vdoryIAAED0k1LSokULy26FCxe2IUOGpHtb9erV3dnDo2ndurW7AAAA5ARr1qwJV31/9tlnrjJczjvvPNu2bVuU1w4AACBGklJZVQkFAACA/1O2bFmbN2+em7BFPTWbNGnilk+fPt3OOussNhMAAIg7J1wppX4H48ePdz0PXnnlFRcwaSjfddddl7VrCAAAEAAauqc+mMnJya4XptokDB482CZOnGjDhw+P9uoBAADERlLqk08+cQ3Ib7zxRvvyyy/t8OHDlidPHnvkkUdcD6dbb70169cUAAAgjjVr1szq169vGzduDM9w3K5dO7vrrruydIZjAACAHJ2U0tm6J5980q6//np39k46duxoJUuWdM3FSUoBAAAc37p1645Ydtppp4WX58+f3w4dOuSua3gfAACABT0ppSF7NWvWTLf5uM7uAQAA4PjUN0qzCqcVCoXcv5G3/fLLL2xSAAAQV04oKXXuuefa119/fURFlGbD020AAAA4Ps2y51FLhAkTJrgWCeonlTdvXvv5559t0KBBrmUCAABAvDmhpJSCpXvuuce+/fZb++uvv1yjc1VP/fTTTzZq1KisX0sAAIA4pEliPK+99pq9+OKLVqNGjfCyevXqWb9+/ezee++1W265JUprCQAAkD1ynciD6tSpY7NmzbIKFSq4svMdO3a44XwzZ860Bg0aZP1aAgAAxLm9e/e6yWPS2rNnjzsJCAAAEG9OuNG5ZoK5//77jwiaVGKuWfgAAACQcS1atLCePXtajx49rGLFiq6v1I8//ugmkbn55pvZlAAAILhJqd9++822bt3q/j9ixAgXLGl2mEj//e9/3Wx8JKUAAAAy3x6hUKFCNnDgQNu2bZtbVqJECWvfvr1rmwAAABDYpNSmTZvsjjvuCF/v1q3bEfcpUKCA3X777Vm3dgAAAAGRJ08ee/DBB93FS0oVL148y//O+vXr7cknn7TvvvvOihYtarfddluqGA8AACDmklL169e3ZcuWuf+rj9SUKVOsWLFi2bluAAAAgeJNHJNeD6lWrVplyd/Q8MCyZcva1KlTbeXKlfbwww+7hutXXXVVljw/AABAtvaU+vzzz2327NmWK1cua9SokVs2YMAA9/9LL730RJ4SAAAg0F5//XV79tlnXXsEDeOLlJCQkCVJqZ07d9qiRYusf//+dtZZZ7mL4re5c+eSlAIAADlj9r2kpCR74IEHbMuWLalKznXm7d13383K9QMAAAiEMWPG2D//+U+bN2+eOwEYefnss8+y5G/kz5/ftVtQlZSqsdQzdOHChVapUqUseX4AAIBsT0opaHruuefshhtuCC/r1auXDR061EaPHn0iTwkAABBoBw8etKuvvjpb/0a+fPmsb9++NmnSJKtRo4Y1bdrUVbm3a9cuW/8uAABAlg3f2759u5UrV+6I5WeffXaq6ikAAABkzPXXX29vv/229ezZ0w3Xyy6//vqrNW7c2O68805bsWKFG8rXoEEDa9GiRYafIxtXLy5424ftFCzsd8QLPruCJyEhes95Qkmp2rVr28svv+ymLFYJuHd275VXXrFatWqdyFMCAAAE2p49e+y9996zGTNm2BlnnGGnnHJKqtvffPPNk/4b6h2lv6HeoBrKV61aNdu4caONGjUqU0mpxMTCJ70uQcB2Cib2O3KyYsVS9zRE/CsW5X1+QkkplX137NjRGjZs6Bpkypo1a6xEiRI2cuTIrF5HAACAuKeY6p577snWv6GZ/cqXL+8SUp7KlSu7E4uZsXXrbguFsmEF44TODisxwXYKFvZ7xuXOnSvqP4SRvu3b91pyckq2bB72e7D2ecL/vguzJSmloXszZ860r7/+2latWuWanCuQUpIqd+7cJ/KUAAAAgdatW7dUVVPJycluJr6sVKpUKVu9erUdOnTI8ubN65ap2bkqszJDCSmSUmwn8P5AfOLzPXhCUTzRdEJJKVEgc8UVV6RapgBHZ+DUOBMAAACZM378eHv99dfDPTqLFy9ut9xyS6qE1clo0qSJm5jmscces3vvvdd+//13VyWlWZUBAAByRFJKUwc/9dRTtnLlSktJSV3mpUopJaYAAACQcSNGjLCkpCS7//77XY9OxViKuYYPH+5OBnbq1OmkN2fhwoVt3LhxNmDAAGvbtq1Leik5ddNNN7GrAABAzkhKPf3003b66afbww8/7AKnIUOGuCaZCpoef/zxrF9LAACAOPfuu++6ZJGqmTyVKlWy0qVLu+VZkZSSc88918aOHZslzwUAAOB7UkrTB6v0u0KFClalShU3O0z79u0tMTHRXnvtNWvWrNlJrRQAAEDQqI+UN4FMpLPPPtu2bdsWlXUCAADITrlO5EEFChQINzQ/55xzbPny5e7/1atXd70JAAAAkDkasjdmzJhUrRHU7FzLFGMBAADEmxOqlKpfv74999xzrkmmAij1Jrjxxhvt888/tyJFimT9WgIAAMS53r17u8rzOXPmuEp0UZ9OTSTzxhtvRHv1AAAAYqNS6tFHH7WdO3faJ598Ytddd52deuqpLlE1cOBA69q1a9avJQAAQJxTW4SPPvrI7rzzTtcSoWzZsta5c2cXb1WsWDHaqwcAABAblVJr1651vaPy5cvnrk+YMMHNxKcqKTXjBAAAQOYtWbLE9ZXq0KGDu64G5wsWLLBLL72UzQkAAOLOCVVKqRoqsndUQkKCnXfeeSSkAAAATpBO8j3wwAO2ZcuW8LI8efJYjx493Mx8AAAA8eaEklJKQOlMHgAAALLG2LFjXc/OG264IbysV69ebsbj0aNHs5kBAEDcOaHhe6eddpo98cQT9tJLL9kZZ5xhefPmTXX7m2++mVXrBwAAEAjbt2+3cuXKHbH87LPPTlU9BQAAEOikVKVKldwFAAAAWaN27dr28ssvu4ljChQo4JYdPHjQXnnlFTfbMQAAQLw5oaRUt27dsn5NAAAAAqxv377WsWNHa9iwoWt2LmvWrLESJUrYyJEjo716AAAAsZGU2r9/v02aNMnNuJecnBxefujQIVu6dKmbzhgAAAAZp6F7M2fOtK+//tpWrVrlmpwrOaUkVe7cudmUAAAg7pxQUuqxxx6zOXPm2MUXX2yzZs2ypk2b2urVq+3HH3+kigoAAOAEqU+nklMpKSl2ySWX2NatWy1XrhOalwYAACA+k1JfffWVvfjiiy4ptWLFCrvjjjusatWqNmjQIHcdAAAAmbNz5067//77bf78+e76xx9/bAMGDLC1a9e62fdOP/10NikAAIgrJ3TqTU03vV4H5513nv3000/u/zfddJN9//33WbuGAAAAAfD000+7Bufffvut5cuXzy1TUupvf/ubuw0AACDenFBSqkKFCm74npeUWrBggfv/7t27XcIKAAAAmaNeUg8++KAVKVIkvCwxMdF69+5t3333HZsTAADEnROefU/l5ep30LJlS7vuuuvsnnvuseXLl1ujRo2yfi0BAAACIL2Te9u2bXNNzwEAAOLNCUU4V1xxhZthT0mpMmXK2Ntvv23/+te/7MILL7QOHTpk/VoCAADEuebNm7vhev369bOEhATbt2+fG8r3xBNPWLNmzaK9ekC2yZUrwV3iRe7cOX9ygpSUkLsAQHY74dNuZ555Zvj/FStWdBcAAACcmJ49e9qwYcOsdevW9tdff1mrVq0sd+7c1rZtW3cbEI+UjCpetKAlxEEix1OsWCHL6ULJKbZtxz4SUwBiMym1fv16e/bZZ23ZsmWuzDwUSp1F/+yzz7Jq/QAAAOLeli1brFixYvbII49Yjx497Mcff7QlS5ZY/vz57YYbbnD/AvGalFJCat/UqZa8eXO0Vweq9CpZ0gq2bu32DdVSAGIyKaWzdZq2WLPtFS5cOOvXCgAAIAD27t1rDz30kM2ePdtmzJjhJpNRi4RHH33UzbqnWfjGjBljb731lrsOxCslpFI2bIj2agAAckJSavHixTZlyhQ38x4AAABOzMsvv2x//vmnJSUl2TnnnOP6SD399NNWvXp1mzBhgp1yyimup5Qq1HUBAACIJyc0eLt8+fKuUgoAAAAn7pNPPnFVUbVr13bNzb/55htXPaWJY5SQEvWY0nIAAIDAVkp999134f83bdrUDeG79957XcNzNeGMdNFFF2XtWgIAAMShzZs3W7ly5cLX58yZ4+Kqhg0bhpeVKFHC9u/fH6U1BAAAiIGklM7YpfX4448fsUxn+X755ZeTXzMAAIA4V7p0aVu7dq2VLVvWTRyj3lI1atSw0047LXyfH374wcqUKRPV9QQAAIhqUkoz7cmqVatc4JQ3b97wbXPnzrVSpUq55pwAjk6zmOgSD3LHwdTNmlGGWWUARFPLli1twIABdv/999u3337rZjhW4/PI+GvYsGHWokWLqK4nAABA1Budq/Hm22+/bePGjbO6deuGl6sR5xdffGG333679erVy1VLAUhNyahixQrFTVJKryWnU0Jq+/a9JKYARI1aIezZs8f69Onj4qfu3btb8+bN3W2DBw+2sWPH2uWXX+7uBwAAENik1Pjx423mzJk2YsSIVAkpGTlypH3++efWu3dv1xfh1ltvzY51BeKiSmraqt229cDhaK9O4CXmz2Mtzirs9gnVUgCiJU+ePC5+0iWtVq1a2fXXX2+VK1eOyroBAADETFLq3XffdT2kGjdunO7tTZo0sYcfftjefPNNklLAMSghtXF/MtsIAHBMF1xwAVsIAADEtQw3hfnzzz+tevXqx7xP/fr1XbNOAAAAAAAAIEuSUomJiS4xdSwbNmywokWLZvQpAQAAAAAAEFAZTkpdddVV9vLLL9tff/2V7u2HDx+24cOHW8OGDbNy/QAAAAAAABDknlJdunSxtm3bWuvWra1Dhw5WtWpVK1y4sO3cudN+/vlnS0pKsr1799qQIUOyd40BAAAAAAAQnKRUkSJFXLPzZ5991gYNGmT79+93y0OhkEtONWvWzO677z4rUaJEdq4vAAAAAAAAgpSUEvWLevrpp61v376uofmuXbvcsnLlylnu3Lmzby0BAAAAAAAQ3KSUJ2/evFahQoWsXxsAAAAAAAAEQoYbnQMAACDnO3TokD311FN20UUX2cUXX2zDhg1z7RgAAAD8lmOSUp06dbJHHnkkfH3p0qXWrl07q1GjhrVp08Z++umnVPefMWOGXXnlle72rl272rZt26Kw1gAAALFFrRjmzJljb7zxhj333HOuZ+ikSZOivVoAACCAckRS6sMPP7TZs2eHr+/bt88lqerUqWNTp061WrVqWefOnd1yWbJkiT366KPWrVs3F2Sp91Xv3r2j+AoAAACib8eOHTZlyhTr37+/Va9e3Ro0aGAdO3a0xYsXR3vVAABAAJ1QTym/g6chQ4ZYtWrVwstmzpxp+fLls549e1pCQoJLQH311Vc2a9Ysa926tSUlJVnTpk2tVatW7v56fOPGjV1z9jPPPDOKrwYAACB6FixYYKeeeqrVrVs3vEwn+gAAAKIh5iulBg8ebC1btrRzzz03vExn82rXru0SUqJ/L7zwQlu0aFH4dlVRecqUKWNly5blLCAAAAg0naA7/fTT7YMPPrBrr73WrrjiChsxYoSlpKREe9UAAEAAxXSl1Ny5c+3777+36dOn25NPPhlevnnz5lRJKklMTLQVK1a4/2/atMlKlSp1xO0bNmzI1N//X84Lx9k+bCfkdBzDGd9GbCvkZNlx/Oa094RaHaxevdomTpxoAwcOdDFV3759rUCBAm4YX7y+br/xmYl4wXs9mNjvwZMQxRgpZpNSBw8etCeeeMIFSvnz50912/79+y1v3ryplum6ZpORAwcOHPP2jEpMLHzC6x8kbCfkZMWKFYr2KuQovN+RU/Fe/z958uSxPXv2uAbnqpiSdevW2TvvvJOppBSfBWwnxD8+N4OJ/R48xaL8eyhmk1LDhw+3qlWrWqNGjY64Tf2k0iaYdN1LXh3tdp0FzIytW3cbMyQfO/OpoJTtlDG5c+eK+hseR9q+fa8lJzNs5Xh4v2cc7/Vgvde990ZOUbJkSRcneQkpOfvss239+vWZeh6++4+Nz8yM4zMzmDES+z12sd+DZ3uUY6Q8sTzj3pYtW9zMeuIlmT7++GNr3ry5uy2SrntD9kqXLp3u7QrEMkMJKZJSbCfEP97nmdtWbC/kVBy7ZjVq1HDV6L///rtLRslvv/2WKkmV0W3J9mQ7If7xPg8m9nvwhELR+9sx2+h8woQJrpeUGnHq0qRJE3fR/xVQ/fDDDxb635bTvwsXLnTLRf9qdhmPzv7p4t0OAAAQROecc45dfvnl1rt3b1u2bJl9/fXXNnr0aLvllluivWoAACCAYrZSKu0Zu0KF/m/YU/ny5V3TcvVCGDBggN18882uWaf6TDVt2tTdR4FVhw4drGbNmlatWjV3PwVgZ555ZlReCwAAQKx49tlnrX///i5eUmuD9u3bu7gJAADAbzGblDqWU0891V599VXXCP3dd9+1Cy64wJ3lK1iwoLtdQ/769etnL730ku3cudMuueQSF3wBAAAEXeHChW3IkCHRXg0AAICck5QaNGhQquvVq1e3999//6j3b926tbsAAAAAAAAg9sRsTykAAAAAAADEL5JSAAAAAAAA8B1JKQAAAAAAAPiOpBQAAAAAAAB8R1IKAAAAAAAAviMpBQAAAAAAAN+RlAIAAAAAAIDvSEoBAAAAAADAdySlAAAAAAAA4DuSUgAAAAAAAPAdSSkAAAAAAAD4jqQUAAAAAAAAfEdSCgAAAAAAAL4jKQUAAAAAAADfkZQCAAAAAACA70hKAQAAAAAAwHckpQAAAAAAAOA7klIAAAAAAADwHUkpAAAAAAAA+I6kFAAAAAAAAHxHUgoAAAAAAAC+IykFAAAAAAAA35GUAgAAAAAAgO9ISgEAAAAAAMB3JKUAAAAAAADgO5JSAAAAAAAA8B1JKQAAAAAAAPiOpBQAAAAAAAB8R1IKAAAAAAAAviMpBQAAAAAAAN+RlAIAAAAAAIDvSEoBAAAAAADAdySlAAAAAAAA4DuSUgAAAAAAAPAdSSkAAICA6tSpkz3yyCPRXg0AABBQJKUAAAAC6MMPP7TZs2dHezUAAECAkZQCAAAImB07dtiQIUOsWrVq0V4VAAAQYHmivQIAAADw1+DBg61ly5a2adMmNj0AAIgaklIAAAABMnfuXPv+++9t+vTp9uSTT57QcyQkZPlqxRVv+7CdkNNxDAcT+z14EhKi95wkpQAAAALi4MGD9sQTT1jfvn0tf/78J/w8iYmFs3S94hXbCTlZsWKFor0KiAL2e/AUi/J7naQUAABAQAwfPtyqVq1qjRo1Oqnn2bp1t4VCWbZacUdnh5WQYjsdX+7cuaL+gwjp2759ryUnp2TL5mG/xy72e/Bsz6b3uvddeDwkpQAAAAI0496WLVusVq1a7vqhQ4fcvx9//LH98MMPGX4eJaRISrGdEP94nwcT+z14QlE80URSCgAAICAmTJhghw8fDl9/9tln3b8PP/xwFNcKAAAEFUkpAACAgDj99NNTXS9U6P+GTZUvX95iRa5cCe4SDzREKadLSQm5CwAA2YGkFAAAAGKCklHFixa0hDhI5kg89EoKJafYth37SEwBALIFSSkAAICAGjRokMVaUkoJqX1Tp1ry5s3RXp3Ay12ypBVs3drtF6qlAADZgaQUAAAAYooSUikbNkR7NQAAQDaLj9poAAAAAAAA5CgkpQAAAAAAAOA7klIAAAAAAADwHUkpAAAAAAAA+I6kFAAAAAAAAHxHUgoAAAAAAAC+IykFAAAAAAAA35GUAgAAAAAAgO9ISgEAAAAAAMB3JKUAAAAAAADgO5JSAAAAAAAA8F1MJ6U2btxo3bt3t7p161qjRo1s4MCBdvDgQXfb2rVr7Y477rCaNWtas2bN7Jtvvkn12Dlz5ljz5s2tRo0adtttt7n7AwAAAAAAIDbEbFIqFAq5hNT+/fvtrbfesueff96++OILe+GFF9xtXbt2tRIlStiUKVOsZcuW1q1bN1u3bp17rP7V7a1bt7b33nvPihcvbl26dHGPAwAAAAAAQPTlsRj122+/2aJFi+w///mPSz6JklSDBw+2Sy+91FU+TZw40QoWLGgVKlSwuXPnugTVfffdZ5MnT7aqVatax44d3eNUYXXJJZfY/PnzrV69elF+ZQAAAAAAAIjZSqmSJUva66+/Hk5Iefbs2WOLFy+2ypUru4SUp3bt2i6JJbq9Tp064dsKFChgVapUCd8OAAAAAACA6IrZSqkiRYq4PlKelJQUS0pKsvr169vmzZutVKlSqe6fmJhoGzZscP8/3u0ZlZBwUi8h7nnbh+2EnI5jOOPbiG2FnCw7jl/eEwAAAHGYlEpr6NChtnTpUtcjaty4cZY3b95Ut+v6oUOH3P/Vh+pYt2dUYmLhLFjz+Md2Qk5WrFihaK9CjsL7HTkV73UAAIDYkyenJKTGjx/vmp2ff/75li9fPtuxY0eq+yjhlD9/fvd/3Z42AaXrqr7KjK1bdxu90Y99dlg/UNlOGZM7dy5+FMWg7dv3WnJySrRXI+bxfs843uvBeq977w0AAADEYVKqf//+9s4777jE1DXXXOOWlS5d2lauXJnqflu2bAkP2dPtup729kqVKmXqbyshRVKK7YT4x/s8c9uK7YWcimMXAAAgtsRso3MZPny4m2Fv2LBhdt1114WX16hRw37++Wc7cOBAeNmCBQvccu92XfdoOJ+G/nm3AwAAAAAAILpiNin166+/2siRI+3uu+92M+upebl3qVu3rpUpU8Z69+5tK1assNGjR9uSJUusbdu27rFt2rSxhQsXuuW6Xfc744wzrF69etF+WQAAAAAAAIjlpNRnn31mycnJNmrUKGvYsGGqS+7cuV3CSgmq1q1b27Rp02zEiBFWtmxZ91gloF5++WWbMmWKS1Sp/5RuT2CKHAAAAAAAgJgQsz2lOnXq5C5HU758eUtKSjrq7Zdddpm7AAAAAAAAIPbEbKUUAAAAAAAA4hdJKQAAAAAAAPiOpBQAAAAAAAB8R1IKAAAAAAAAviMpBQAAAAAAAN+RlAIAAAAAAIDvSEoBAAAAAADAdySlAAAAAAAA4DuSUgAAAAAAAPAdSSkAAAAAAAD4jqQUAAAAAAAAfEdSCgAAIEA2btxo3bt3t7p161qjRo1s4MCBdvDgwWivFgAACKA80V4BAAAA+CMUCrmEVJEiReytt96ynTt3Wp8+fSxXrlzWq1cvdgMAAPAVlVIAAAAB8dtvv9miRYtcddR5551nderUcUmqGTNmRHvVAABAAJGUAgAACIiSJUva66+/biVKlEi1fM+ePVFbJwAAEFwM3wMAAAgIDdtTHylPSkqKJSUlWf369TP1PAkJ2bByiGns82BivwcT+z14EhKi95wkpQAAAAJq6NChtnTpUnvvvfcy9bjExMLZtk6IPcWKFYr2KiAK2O/BxH4PnmJR/ownKQUAABDQhNT48ePt+eeft/PPPz9Tj926dbeFQlm/Trlz54p6cIwjbd++15KTU7Jl07DPYxf7PZjY78GzPZs+41UplZGTWCSlAAAAAqZ///72zjvvuMTUNddck+nHKyGVHUkpxC72dzCx34OJ/R48oSh+p5OUAgAACJDhw4fbxIkTbdiwYXbttddGe3UAAECAkZQCAAAIiF9//dVGjhxpnTp1stq1a9vmzZtTzcwHAADgJ5JSAAAAAfHZZ59ZcnKyjRo1yl0iLV++PGrrBQAAgomkFAAAQECoQkoXAACAWJAr2isAAAAAAACA4CEpBQAAAAAAAN+RlAIAAAAAAIDvSEoBAAAAAADAdySlAAAAAAAA4DuSUgAAAAAAAPAdSSkAAAAAAAD4jqQUAAAAAAAAfEdSCgAAAAAAAL4jKQUAAAAAAADfkZQCAAAAAACA70hKAQAAAAAAwHckpQAAAAAAAOA7klIAAAAAAADwHUkpAAAAAAAA+I6kFAAAAAAAAHxHUgoAAAAAAAC+IykFAAAAAAAA35GUAgAAAAAAgO9ISgEAAAAAAMB3JKUAAAAAAADgO5JSAAAAAAAA8B1JKQAAAAAAAPiOpBQAAAAAAAB8R1IKAAAAAAAAviMpBQAAAAAAAN+RlAIAAAAAAIDvSEoBAAAAAADAd3n8/5OQXLkS3CUe5M4dH7nNlJSQuwBZKZ7e6/Hyfue9DgAAAMQGklJRoB+oxYoVipsfqnot8UA/VLdv30tiClkm3t7r8fJ+570OAAAAxAaSUlGsnJi2ardtPXA4GquANBLz57EWZxV2+4VqKWQV3uuxh/c6AAAAEDtISkWRElIb9ydHcxUA+ID3OgAAAAAcKec3BwEAAAAAAECOQ1IKAAAAAAAAvovbpNTBgwetT58+VqdOHWvYsKGNGTMm2qsEAAAQdcRIAAAgVsRtT6khQ4bYTz/9ZOPHj7d169ZZr169rGzZsnbttddGe9UAAACihhgJAADEirhMSu3bt88mT55sr732mlWpUsVdVqxYYW+99RZJKQAAEFjESAAAIJbE5fC9ZcuW2eHDh61WrVrhZbVr17bFixdbSkpKVNcNAAAgWoiRAABALInLpNTmzZutWLFiljdv3vCyEiVKuB4KO3bsiOq6AQAARAsxEgAAiCVxOXxv//79qRJS4l0/dOhQhp8nVy6zUMiyTekCeeyUuEwL5jzF8+VJtd+zE/s9NrDPg4n9HjzZvc8TEixHySkxUu4yZcxOOSX7/gAyth9KlPAtPmKfxw72ezCx34MndzZ/xmc0RorLpFS+fPmOCKy86/nz58/w8xQvXtiyU7Py2fv8yLxixQpl+2Zjv8cW9nkwsd+Dx499nhPklBipYIsW2fr8iL33D/s89rDfg4n9HjzFohwjxWWdTunSpW379u2ur1RkubqCrSJFikR13QAAAKKFGAkAAMSSuExKVapUyfLkyWOLFi0KL1uwYIFVq1bNcmV37TEAAECMIkYCAACxJC4zNAUKFLBWrVrZk08+aUuWLLFPP/3UxowZY7fddlu0Vw0AACBqiJEAAEAsSQiFsrNNZXQbeSop9cknn9ipp55qd911l91xxx3RXi0AAICoIkYCAACxIm6TUgAAAAAAAIhdcTl8DwAAAAAAALGNpBQAAAAAAAB8R1IKAAAAAAAAviMpBQAAAAAAAN+RlAIQMw4dOhTtVQBwFNu2bWPbAEAUEB8BsY0Y6eSQlAIQE/bt22ffffed7dq1K9qrAiCNRx991Lp168YPIwDwGfERENuIkU5enix4DgA4aWvWrLGhQ4fafffdZwkJCXbgwAFr1qwZWzZgUlJSLFcuzpfEkgEDBthnn31mo0ePtrx580Z7dQAgUIiP4CFGij3ESFmDpBSyXCgUckmF3377zTZu3GhFihSxs88+2woWLMjWxlFVrFjRGjRoYE888YRt2bLFXnjhBbZWgIOtTz/91Pbs2WN58uSx5s2bR3vVAmv48OE2YcIE++STT6xcuXL2119/2SmnnBLt1QJyLGIkZBbxEYQYKfYQI2UdTkcjSwIsfVB6lJD6+OOP7bbbbrN+/frZoEGDbOnSpe5+ugBpHT582P3bsmVLO3jwoBUtWtQdU0pKIDi8hJQ+M/r27WtJSUnuDLESIR4+Q/wzePBgF3DppMLXX3/tlikhFfl5D+DYiJFwMoiP4CFGii3ESFmLSimcNP1AyZ07d/j6qlWr7JlnnrEePXrYdddd535Uli9f3iWr1KiR4R844oMoTx5buHCh7d6928aNG2czZsxwlVIawnf11VfbqaeeykYLCFXkzJo1y9555x0744wzbMeOHbZp0yZXedmoUSP3OeJVGiD7DBw40KZMmWKvv/66O6nw/vvvu4Rxx44dXWDMEAIgY4iRcDKIjxCJGCk2ECNlPZJSOClKIEybNs2mTp0a/pGiRIKG7F188cVWoEABu+CCC9x9165da2+//bbde++97nbAk5ycbA899JC1bt3a9ZSqUqWKW/bKK6+45MNVV11FYiog9PmRmJhoZ555pi1ZssRVS82dO9clLOvVq2evvfYaCalstmzZMvv+++/dttewkQoVKriE1EcffeQSgnfddReJKSADiJFwsoiPEIkYKfqIkbIHSSmclCZNmthll13m/u9VLmj2tJUrV7p/y5Yt60qPdaZn+fLl7kdN+/btSUohVbWLKu3q16+faua9Pn36uNuVmNq7d6+rmFHlTKtWrdh6cSK9apu//e1vbtimmtyr6lKfMV26dLGaNWva3//+d/v222/dsYLso0TU+PHjXSJY+6hMmTJ24403uttUxSYkpoDjI0bCiSA+ghAjxSZipOxBUgonRY1vRUOvevfu7aqm6tat6ypb1LBa5Y3nnHOOu0/t2rVdMkoVD4ASTqrGyJcvn6vEqF69uqukUwKqUKFCbgPpmNJZwnfffde2bdvmzjoj/oItVURpaK+OBX1+PPbYY+7YUCLqwgsvtNNOO80N4VNvI4Zy+rNf9B7UDyP9X/+WLl2axBSQScRIOBHERyBGik3ESNknIUTXWGSB1atXW+fOnV0TXA3lU4XDc889Z5s3b3bJKf2QVMLqww8/tIkTJ7rhOQg2HRudOnVyPcdUSVeqVCmXmLjnnnuscuXKdvrpp4cTEBs2bHDHFsdNzpe2H9TQoUNt9uzZrpm59rcS16qOU4Jq69atrsG2KuTU20jHjBKUkT3skL37J21wrBlVtQ+++uorV8125513sguA4yBGQmYQHwUXMVJsI0bKPiSlcFJvSlU9aXhesWLFbN26da4fkH5cKjH1559/2ksvveSmdle1lO6r60o4AN4P3V9//dVVwSxYsMAlI5SA+OOPP+z888+3/Pnzu+F63tAhxNe+/+CDD9xMe2PHjrVKlSq5Sjhdnzx5slWrVs0lPkaOHOk+OzSEbNSoUS45qeo5ElPZ97n+ww8/2I8//mj79++3OnXquCrXSEpMvffee+5Eg4ZUdujQIRvWBsi5iJFwMoiPgo0YKTYRI2UvklI44Telkk36wbhlyxb3w0RVLzob+MADD7iKFyWmNNPezz//7CogNBykRIkSbPGAHzcrVqywnTt3uh5RV155Zfh2/QBWtZ36BakPh+738ccf24MPPhgeAoqcq3v37m4/alZOj5JQ6jWnYb6aUUbDNdVL7LzzznO9o/SZomGb+jzR8DEdP16POmTt+zLyc71nz55usgGdYFi0aJF169bN2rVr5/aBZ/369TZ9+nRr2rSpa0oPIPV7ihgJmf0cJj4KLmKk2ESM5CMN3wMya86cOaFq1aqFXnnlldB//vOf0K+//hq+bfXq1aGbb7451KxZs9D+/fvZuAibNWtW6PLLLw+1bt06dO2117pj5JtvvgkdPHjQ3f7qq6+GOnbsyBaLM4cPHw6NHTs2VKVKldBrr70WXj5o0KDQvffeG1q4cGGoVq1aoaSkpPBxcs0114TWrVuX6nmSk5N9X/cg2bBhQ6hly5ahyZMnh5dNnz49VK9evdBLL70U3gcpKSnh/QrgSMRIyCzio+AiRsoZiJGyF6ebcUJlpf/5z3/spptucpUtGlqjKodhw4a52zWUY/DgwXb33Xe7/2soDqBhegMGDHCVdDfccIMb7qmKKFVN6ZgSzdaoYUORzc6R82monWbd1D5Vjzntb1VB3XLLLXb77be7f59++mlr27atu7/6SakaSv9GSjtTH06chkaqn5sq0c466yxr0aKF2y+qWDz33HPDZwebN2/ulvfq1csaNWrkms9H7lcAqREjIbOIj4KNGCn2ECP5j6QUMk0/DHXR8Dz1fXnttdfccjWmVoJBCalJkybZG2+84X7YIHjWrl1ru3btckOAIht3qveYElL6MawGyRoSpFn3nn/+eTd0S8eTjqM9e/aQlIoTXnJDvaBatmzpfrA9+eST7jYlppS81qyLv//+u0twq7m5JkPQcaDjBVnv2WefdUNjNRxPQ/T+/e9/W7169dz7Tv3dDhw44PbZwYMHXWJQCatXX33VDeWLTEoBOBIxEo6F+AiRiJFiDzFSdJCUQoY/MPVjUf1c9ENG1Q1Lly51Py4vv/xyVzWlHzXqKdWlSxd39l0NqxFM6hc1ZMgQe+SRR1yAribW+nGr3mI6I/iPf/zDGjZsaP3793c/hNXw+sILL7TGjRu7JCe9x+JD5Kxtoh5z+qzQZ8pTTz3ljgl9lmi5+tMp0V20aFF3nChRpc+dtM+Bk5OUlOQalL/88svufaltr/erZj3UZ/t1113nekq9+eabroJK1NNLVW7Fixdn8wNpECMhM4iP4CFGij3ESNFDUgoZbtip4XkqMT377LPdLHreFO0lS5Z0Z9RFU4XrR07BggXZsgGmqhg1tX744YddBYxXVaem+PrRq8b4jz32mLuvhurpGFJVjI4dElLx89nhJZOU4FBDc1XPqVJOQ8J0jDz++OPudiWmVI0zb948l5TSDJ36rKGpedbuDwXA3333nd11111Wo0aN8G3a5t4QWg3J1na/+eab3ZBKJQ6///57NyNmrVq1snCNgJyPGAmZRXwE77ODGCl2ECNFH0kpHJMSUl9++aXrA6ThVko0qKzxvvvuc2falUxQRYOqW84//3w3vEPTuxcuXJgtG2AVK1Z0w3w0jLNcuXIuGaEeQcOHD3czeekH7qpVqyw5Odk++ugj27dvn7sf4u/s36hRo1xS6pprrnHHgD47OnbsaDfeeKOrllKPKSW1NZRPPYs8OjaYZS9rP8s1PHLhwoXWunXrVD+oxdtf6ulWu3Zt93muStjTTjvNJYt1EoJZ9oAj31fESMgM4iMQI8UeYqToS1C382ivBGKTfhTqx6KSCHXr1rV77rnH9RvR8JsNGza4ZePHj3fDr5SYUqWLlpUvXz7aq44YoB+/K1eutF9++cUN9bz33nvdUM///ve/dv/997shQfoS0I9h9ZSK7D+F+KB9P27cONfQXkkp0aQIffv2dUluVeUocanElIZ7qloK2Udf99dff71rKH/HHXeke59vvvnGDa9Vnyklo3RWX+9RVVMB+P+IkXCiiI8gxEixhRgpuqiUQpjOkOsHiBIFF1xwgRs+o/+rikVnyJV80rA99f1Rg2oNuXnooYfcMCxVOSDYvKoL9YxSTzH1q1GfKCWhlJhQzyDdftlll7nqKCWqFNT/7W9/cxV3iK+zgNrvmlVPTbMjh31dffXVLtmtxvZKVinJrf1/6aWXRnWdg/D+VCJYvf7mzp1rbdq0SbeiVRMPaAithlteeeWVUVlXIBYRI+FEER8hEjFS7CFGij66x8LRkDz1/1EFiypaNHOeFChQwA3D0hCO+fPnu8SUqhk0jE+Jqg8//NBVUHk9pRBcSjjNmjXLJSK6du3qegepF5mGAakq5rzzznNDuVSBoeWaXU19pkhIxQevJ5GoskZDFPS5kpiY6M4GKlHpUbJDyRHNwihKTmmonnoZIfven+oPpYko/vOf/4RnTU27/7Zv3+4amtPUHPj/iJFwsp+/xEfBRowU24iRoo9KKdgzzzzjGlGrZ4iqo5YtW+YCMDU0149FzaAmanqbP3/+8JTgSjIoiXXuuee6HzsINs28qOGcOiYaNGjg+ke98MIL7uzDVVdd5YYLjRkzxg3VUoJCAZqSnYivhp0zZsxwySZVVCr5pNsGDBjgZnJr2bKlS3aoQk6fNbpEoodU9lMllIZPql+UKqeUMNase97+mzx5sttnDMMG/g8xEk4W8VGwESPlHMRI0UNSKuDUx+Vf//qXvfXWW27InigZpcadak79119/uSF93jS26im1fv16N2Wmzrb36NHDDb9CsGnInqqgNOtiq1at3PTxmq3xn//8p7344ovuC1nDtpTg1HAtDQ9SpQzig9csW73B1NRc1W9KUGr/q3+RklADBw50s+upwlJJK32uXHLJJdFe9UDSPtGJBCWIlyxZ4obaKkG8du1a+/rrr91kFapwA4KOGAkni/gIxEg5CzFSdNDoPMDWrVvnKqF69uzpZsPyqhdEVS1bt251Z9KVrOrQoYOrcNBU4voxqWE2I0aMcFO3I7i8GUSmTZvmmt2vWLHCZs6c6SovPEpMqL/Q3Xff7SqmqKqLzxlk9u7da4MGDXIzu6mHlKot1TtMw3vVg+7zzz93SclSpUq5pueahU8iE9/w/8eSKqPUK0f7UZ/1t956qxueDQQdMRJOBvERiJFyNmIkf5GUCrivvvrKNSvv37+/+6GobP7o0aNdU2olqlTxoCbVmnlJQ69k1apVrjqKM+nB5TXtjPzCVRXM4MGD3SyMGupZpkyZ8P3Vx0ZD9jRE9NRTT43imiOrjwGZPn26q6BUguO5555z5c9elYGGaapPnRJTs2fPdtU5GsanxLeOFcTmPgVAjIQT/xwlPgo2YqT4Q4yUvUhKwSWmunfv7hIGmqZWTc5V5dCoUSO3dTSkQxUuGprTtGlTtljAeR/KmsFL1S9qcq/KCiUZtEzHkXqPqW9NZMXUxo0bU11HfHwxq3eYLnXq1LHvv//eHn/8cWvfvn34vkOHDnXN7bVMM3YqgaVeY6rS/Mc//sExEUP7koALOBIxEjL7eUp8FGzESPGDGMk/zL4HNxW7+v78/e9/d0PyvISU3ogappc3b17Xc0TVUoACri+++MJVv+zbt882b97sKmQ0bEszrqnCTsvV3FrVMx4SUvHDS2KoOk6ztb3zzjv2yiuv2KOPPuqq5HQ8RA7frF+/vi1YsMB9plx//fWuck5BO8P2oi+yMooqKeBIxEjIzOcp8RGIkeIHMZJ/qJRCmH5gqpJBPy4bNmwYngnr5ZdfDjdDJ7EQbOo7duDAAdcnSInLTp06haugVG2nJOaUKVPsm2++cdUzamauIX1pZ1lDzrd06VLr06ePS0oqKVWuXDm3XMN9tc/79evnhuwdbUjDnj17GMoJIMcgRsKxEB8hEjESkDlUSiGsXr169uqrr7oZ9XSmx0tIvfbaa66SioRU8CgBpVkXlUAQJZeUXNiyZYudd955bpmu69hQhZ3up1nXlNTU0Cz1KyMhFR+UTIqkhuWqjlNV3HvvvRdermGcamj+1FNPuZn4PEpIRZZBa4ZGAMgpiJEQifgIkYiRgJPzf6UwwP9cdtllLgGlITeqeNGZQc2qVqVKFbZRwKg31A8//OCGXRUuXNhVRanyRY3KNYOeEpeNGzcOJxnU/L5EiRL2xx9/uOtXXnlllF8Bskpkw1b1jSpQoIDb17fddlv4WNEw327durnrqrhUskpNzjVzp3eMUAYNICcjRoIQHyESMRJw8hi+h3RplqzOnTvb1KlTrXLlymylgBk2bJjb9/fff7+rblEz84suusjOP/98l5zQUM5p06a5xveqjPHcd9997nhRvymaJseHyP2opuWqiipYsKCdddZZ9sADD7iZ9iZMmOBm52zbtq117dr1iMdyLACIJ8RIwUV8hEjESEDWICmFo9q/f7+riECwqAF1//79beDAgVajRo1wrwRv6J6SDFu3bnVnClVFpeREgwYNbNGiRTZjxgyXtDj77LOj/TKQxdRrTv2iXnrpJXdWcOTIkS5ZqebmSkwlJSXZ2LFj7eqrr7ZevXqFH0dCCkA8IkYKHuIjHA0xEnBy6CmFoyIhFUwbNmyw008/3c24qGSUeH2hvIoZNTa/++673dCsNWvWuMqptWvXun9JSMWfQ4cO2cKFC12vqLp167rPhpUrV1r+/Pld8vLnn392s3fedNNNbvimElEeZnQDEI+IkYKH+AjpIUYCTh49pQCk8vvvv9uuXbtcj6D0qPF5z549XX8p9Q66/vrrXcNPJR/Uawo5X2R1k6qhFHAp8XTDDTe4Kjk1MFePsVq1arkZ+J588kk3ZFPLGLIHAIhHxEcQYiQg61EpBSAVNbBW4sGbcc+rlhIN2ypatKhLRmi4nnebKmZISMUPLyE1atQo1ztMje6VeCpTpox9++239tdff1n9+vXd8M7ixYvb7t273ayL3mMZsgcAiDfERxBiJCDrkZQCkIoqnzRz2uDBg8ND97zkkzcDm5JT27ZtCw/rQ3xavHixm4FRLr30UqtZs6brG3baaadZxYoVbe/eva6iTg3P1W/Kw5A9AEC8IT5CJGIkIOuQlAIQpmRTsWLFrEePHvbpp5/as88+65Z7ySclp3QfDddTc2vEj8OHDx+xrGrVqvbnn3+6/3uVcNWqVXOJqkmTJln37t1dYurKK690iSgdGwAAxBvio2AjRgKyF0kpAP//A+F/lVBXXHGFtW/f3t599117+OGHbfXq1S4RtX79env55Zdtzpw51qpVK7ZcHFAfKCWe8uTJEx6y169fP5s5c6adccYZroeY9rtHx0blypXt7bffdsnKCRMmhKvpvOMHAIB4QnwUTMRIgD8SQpHTJAHA/ygZMX/+fDeMTx8T6jF11llnuWmwhw4d6oZvIWfT7IlKOE2fPt3NJLV9+3YbPXq0q4TS8EwN41R/sauvvtrOPfdcu+yyy+z888931VHqM3XKKae4CimdQfSSWgAAxDPio2AgRgL8Q1IKCGgZupIJaXv/aHnaahc1tVZySskLJaXKlStnpUqV8nmNkdVuueUWN7PeW2+95RJSafe9Ek0LFiywAQMGuAbnGzdudMmotWvXWufOnV0fqaMdMwAA5ETERxBiJMBfJKWAAFEljHpGeZYsWWKbNm1yzarVyFo0DCuyhxTNzOM32EpKSrKCBQvaoUOH3DEgmklPVVCePn362K5du+zFF190Z4eVqGrSpAmVUQCAuEF8BA8xEuA/Tm8DATF58mRr06aNrVy50l1XI3OVJj/zzDP21FNPuUva2fZISMVvsDVx4sQjElKPPvqoa2Cu4ZreMdCwYUNXJaVjITEx0Q3l01C99Jp+AgCQ0xAfwUOMBEQHSSkgINq2bWv58+e3Xr16uWls33zzTevfv7+NHz/eunXrZt999509/vjjRySmED82b95sa9ascdVySkQp+eQlpDST3vfff2/Nmzd3wzq9hKR6h/3444+uqi4SPaQAAPGA+AhCjARED0kpIM5pZrWff/7ZJRo+/PBDl4jo0aOHm02vevXqduaZZ9o111xj99xzjxua9dhjj7nHkZiKPyVLlnQ9pH7//Xe76667wj3FlJDSsjFjxtjf/vY3d4x4vTX0/5tvvtmqVKkS5bUHACDrEB8hEjESED30lALinKpf3n77bevatasVKVLEfeneeeedNnfuXBs3bpzVr1/f3U8zrX3xxRf26quvWoUKFez555+P9qojm6xatcruuOMOq1y5sqt40vWRI0faGWec4ZJQXrLq/vvvd2eQNYRPy+gxBgCIF8RHSA8xEuA/KqWAOKXZ8kS9f/bv3+/6BTVq1Mj++OMPGzt2rFWqVMnNrOb1mFJ/ocaNG7uE1bp161wDdMQnzaKohKRm0vvkk09cE3MlpLxZh0SVcwrYlbT0ltFjDACQ0xEf4ViIkQD/kZQC4tCUKVNcJYyaWCupUK9ePVu0aJEbgqUZ1OT99993yYaHHnrIfv3113BiSkP53njjDStVqlSUXwWyO+gaPny4lS1b1gYNGuQSl7ly5XKVUnfffbcbzvfll1/aKaecQlNzAEBcID5CRhAjAf5i+B4QZzSr3pVXXukaNmqo3p49e9ywPFU+KTGlRFT79u1dokpatGjhEg+ahe+CCy6I9uojCmXqqo7Tvn/hhRdcvzElpGbMmBFOSNHUHACQ0xEfIbOIkQB/UCkFxBFVPA0ePNhVOhUvXtzNmNamTRvXzFyNrTXVraqn1Ox63rx57jHTpk1ziau+ffu62xAflExKT9pZFXU2UMM5V6xYYTVr1nSz85GQAgDEE+IjRCJGAmILlVJAHDl48KBLSq1evdqGDh3qElNKSu3cudP1DdLwvTlz5tiECRNcFYwqpvLmzWu1atVys9Ccfvrp0X4JOEkbN250FXIaiheZdCxcuLBdf/31bll6DcsVsGv2vSeffJIKKQBAXCE+ghAjAbGJpBQQZ3bv3u2SD5ox7emnn3bLbrvtNtfUWj2ElJjSzHtedYwafn722WckpOLAwIEDbebMmTZ+/Hg755xz7Nlnn3UzL5YpU8ZVwV188cX21FNPufseayY9huwBAOIN8VGwESMBsYukFJDDqWG5qmJatmwZXjZ//nx75JFHrHv37taqVSu3TI3PVUHlJabUN0hDtc4++2wrV65cFF8BsoqalGt/K9n08MMP26hRo9ywzKJFi7pjYuTIka4qzktWHisxBQBATkZ8hEjESEDsIikF5GA//vijtWvXzg3Tq1atmks2FCtWzCWpnn/+eduyZYt16dLF9ZTyElMapqcKmho1akR79ZFFfvnlFzc0QT2hFHTdeOONLuGomfVeffVVN5Pi3r173Wx6I0aMsNq1a1v//v3dY1NSUsJD/QAAiAfER/AQIwGxj18iQA6mZMO1115rl156qUsu3HzzzW7olobkKVm1fPlyF5h5xo0bZ0WKFLHHH3/cJTEQH9QPSs3tNRxTFVGTJ0+2SpUquUBsw4YNLlFVqFAha9y4sXXr1s3NwqgqOiEhBQCIN8RH8BAjAbGPSikgh/vwww/tueees3fffde++eYb+/e//+2qoZ544glbuXKla3g+ZcqUcLWUrFu3zlXRIOcHWhUqVHD7feLEiS4ppX3/008/uds1lE/JymHDhtm5557rlu3bt88++ugj11dsyJAhJKUAAHGJ+CjYiJGAnINKKSCH+eKLL2zhwoXh69ddd501aNDAHnzwQWvatKn16dPHrrnmGtfcfPv27S4Z8frrr7tkhIeEVM6n4ZneEDw1tVfSUT3DLrroonB13AcffOD+feihh1xwJgULFrTmzZu7IZyqklLSCgCAnI74CB5iJCBnoVIKyEHUI6pjx45udrS77rrL2rRp45ZrZr1BgwbZ5Zdfbm3btrWEhAT7/vvv7c0337Q//vjDVUyNGTPG6tSpE+2XgCygIZq33357qimOf/jhBzfDnirltP/bt29v9erVc7erCb4amj/zzDNWsWJF9gEAIK4QH8FDjATkPFRKATlIiRIl7LXXXrPrr7/eHnvsMTezmoZqqUqmevXqrlRdiQlRAkr3UaNz9RcqXbp0tFcfWWDBggUu4NJwTVU5/etf/3JJp9NPP91atGjh+orpGHjrrbds3rx57jG6z549e1zFHAAA8Yb4CEKMBORMVEoBOdSSJUtswIABbuY9VcRoZj01N7/gggvcLHyRkpOTXaUMcr4DBw5Yv379XHXUqFGjLG/evHbrrbe66y+99JJVqVLF5syZYxMmTLA8efK4vlK7d+92/3IcAADiHfFRcBEjATkTSSkgB9PMahquNW3aNEtMTHRVMiNHjnTVURrKh/i0c+dOa9asmesdpmo5UQ8xDeMcPny4S0ypkbmGbyo413C+2bNnu8QkiSkAQLwjPgouYiQg5yEpBeRw6i+loVmPPPKI66mg/9eoUcPNvqem1sj51JT8jDPOcElHz1dffeUq5e6//36XoBJVy6nZuZeYUlCuCqqqVau6hJSG+6m5OQAA8Y74KBiIkYCcj6QUEEc025ouv/zyi3388cdWtGjRaK8STtKXX35p99xzj5UsWdLOOuss16y8VKlSrvpp6NCh9tdff9m9994b7hl255132po1a1yQVqtWrfDzUCEFAAgq4qP4RIwExAeSUkAciKyAUbWU1/QTOZ+G5A0cONDKlStn69evdwlHNTTXRYmmbt262UMPPWRNmjQJP0b9o3R/9ZgCACCoiI/iGzESEB9ISgFxIhQKueoZxJ8pU6a4BNP06dPtm2++sU8++cQlp9TQ/ueff3az6k2ePNnKlCkTfgxD9QAAID6Kd8RIQM5HcxEgTpCQih+aOW/+/Pnh623atLH69etbz5497corr7THHnvMbrrpJuvUqZPt2LHDDd1TU3PNOuNR5ZwqqQAACDLio/hCjATEH5JSABBD/vjjD3fWT4mniRMnhpd37NjRJZo026KGZur62LFjXTPzgwcPWlJSkq1atSrVc6m5OQAAQDwgRgLiE8P3ACDGbNu2zaZOnWrDhg2zli1bWrt27ezCCy+0F154wRYtWmTjxo1Ldd9ly5bZ+++/b4MGDSIRBQAA4hYxEhB/SEoBQIxaunSpDRkyxPLly+eSUp07d3ZJqosuushVUqWHWfYAAEC8I0YC4gdJKQCIYZs2bbKvvvrKDeUrXLiwS0pNmjTJunfvbg0aNIj26gEAAEQFMRIQH0hKAUAOmFnx0KFDrtH5+vXrbd26dXb55Zdb3759LW/evNFePQAAgKggRgJyPpJSAJADAi5v9qBZs2bZ5MmT3Ux7am7OrEIAACCoiJGAnI+kFADksKBr586dVqRIEXc9cjkAAEDQECMBORtJKQDIIdImoFJSUixXrlxRXScAAIBoI0YCci6SUgAAAAAAAPAdp9gBAAAAAADgO5JSAAAAAAAA8B1JKQAAAAAAAPiOpBQAAAAAAAB8R1IKAAAAAAAAviMpBQAAAAAAAN+RlAIAAAAAAIDvSEoBAAAAAADAdySlAAAAAAAA4DuSUgAAAAAAAPAdSSkAAAAAAACY3/4fEaK6G6pQZvYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Prompt Performance Visualization: Quantitative Comparison\n",
    "\n",
    "This example compares different prompt variations across multiple metrics:\n",
    "1. Response Length: Measures verbosity/detail level\n",
    "2. Response Time: Measures latency/processing time\n",
    "3. Response Quality: Can be extended with accuracy metrics\n",
    "\n",
    "Useful for:\n",
    "- A/B testing prompt variations\n",
    "- Optimizing for specific metrics (speed vs. detail)\n",
    "- Understanding trade-offs between prompt styles\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "\n",
    "# Define different prompt variations to test\n",
    "test_prompts = {\n",
    "    \"Basic\": \"What is the capital of France?\",\n",
    "    \"With Context\": \"Given that Paris is a major European city, what is the capital of France?\",\n",
    "    \"With Examples\": \"\"\"What is the capital of France?\n",
    "Examples:\n",
    "- Capital of USA: Washington D.C.\n",
    "- Capital of UK: London\n",
    "- Capital of France:\"\"\",\n",
    "    \"CoT\": \"What is the capital of France? Think step by step and explain your reasoning.\"\n",
    "}\n",
    "\n",
    "# Store results for each prompt variation\n",
    "results = []\n",
    "\n",
    "# Test each prompt variation\n",
    "for prompt_name, prompt_text in test_prompts.items():\n",
    "    # Measure response time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate response\n",
    "    output = generate_response(\n",
    "        deepseek_model_client,\n",
    "        \"You are a helpful assistant.\",\n",
    "        prompt_text,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    # Calculate elapsed time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Extract final response\n",
    "    response = output.strip().split(\"</think>\")[-1].strip()\n",
    "    \n",
    "    # Store metrics\n",
    "    results.append({\n",
    "        \"Prompt Type\": prompt_name,\n",
    "        \"Response Length\": len(response),  # Character count\n",
    "        \"Time (s)\": round(elapsed_time, 2),  # Response latency\n",
    "        \"Response\": response[:100] + \"...\" if len(response) > 100 else response  # Preview\n",
    "    })\n",
    "\n",
    "# ============================================================================\n",
    "# Display Results Table\n",
    "# ============================================================================\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"=== PROMPT PERFORMANCE COMPARISON ===\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# Create Visualizations\n",
    "# ============================================================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Response Length Comparison\n",
    "axes[0].bar(results_df[\"Prompt Type\"], results_df[\"Response Length\"], color='skyblue')\n",
    "axes[0].set_title(\"Response Length by Prompt Type\")\n",
    "axes[0].set_ylabel(\"Characters\")\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Response Time Comparison\n",
    "axes[1].bar(results_df[\"Prompt Type\"], results_df[\"Time (s)\"], color='lightcoral')\n",
    "axes[1].set_title(\"Response Time by Prompt Type\")\n",
    "axes[1].set_ylabel(\"Seconds\")\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "exp",
   "language": "python",
   "name": "exp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
